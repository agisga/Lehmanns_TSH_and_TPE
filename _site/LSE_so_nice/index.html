<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>Least squares estimators are nice! PART 1 (UMVU, MRE, BLUE)</title>
    <meta name="description" content="A collection of random facts I observed while reading Lehmann's  "Testing Statistical Hypotheses" and "Theory of Point Estimation"
">

    <link rel="stylesheet" href="/Lehmanns_TSH_and_TPE/css/main.css">
    <link rel="canonical" href="http://www.alexejgossmann.com//Lehmanns_TSH_and_TPE/LSE_so_nice/">
 
    <!-- MathJax interation-->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [["$","$"]]}});
      MathJax.Hub.Config({TeX: {Macros:{subscript:['_{#1}',1],superscript:['^{#1}',1]}}});
    </script> 
    <!-- Turn on equation numbering -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
    </script>
    <script type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full">
    </script>
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/Lehmanns_TSH_and_TPE/">Some impressions from the books  "Testing Statistical Hypotheses" and "Theory of Point Estimation"</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Least squares estimators are nice! PART 1 (UMVU, MRE, BLUE)</h1>
  </header>

  <article class="post-content">
    <p>The well-known least squares estimator (LSE) for the coefficients of a linear model is the &quot;best&quot; possible estimator according to several different criteria. Three types of such optimality conditions under which the LSE is &quot;best&quot; are discussed below. In the process, we also briefly look at the &quot;best&quot; estimators of the variance in a linear model.</p>

<p>Let&#39;s fix the concepts first, and then explore how they apply to LSE.</p>

<h2>Some definitions and implications</h2>

<h3>UMVU estimators</h3>

<p>As one would expect, a <em>uniform minimum variance unbiased</em> (or UMVU) estimator $\delta(x)$ of $g(\theta)$ is an unbiased estimator such that
$\mathrm{Var}\subscript{\theta} \delta(X) \leq \mathrm{Var}\subscript{\theta} \delta^\prime(X)$ for any other unbiased estimator 
$\delta^\prime(x)$ of $g(\theta)$ and any $\theta\in\Omega$.</p>

<h3>Invariance</h3>

<p>Let $X \sim P\subscript{\theta}$ for some $\theta\in\Omega$. That is, $X$ is distributed according to one of the distributions in the family $\mathcal{P} = \{ P\subscript{\theta}, \theta \in \Omega \}$ of distributions. Let $G$ be the group generated by the set of all bijective transformations of the sample space of $X$ onto itself.</p>

<p>If for any $g\in G$ it holds that $gX \sim P\subscript{\theta^\prime}$ for some $\theta^\prime \in \Omega$, and if as $\theta$ traverses $\Omega$ so does $\theta^\prime$, then $\mathcal{P}$ is <em>invariant</em> under $G$ (Definition 2.1 in Chapter 3 TPE).</p>

<p><em>The principle of invariance has some interesting implications:</em></p>

<ul>
<li><p>If $G$ leaves $\mathcal{P}$ invariant, then there must be a bijective transformation $\bar{g}$ such that $\theta^\prime = \bar{g}\theta$. Such transformations $\bar{g}$ form a group $\overline{G}$, and we have that</p>

<p>$$
\begin{eqnarray}
\nonumber
P\subscript{\theta}(gX \in A) &amp;=&amp; P\subscript{\bar{g}\theta}(X \in A) \\\
E\subscript{\theta} \psi(gX) &amp;=&amp; E\subscript{\bar{g}\theta} \psi(X),
\label{eq:invariant}
\end{eqnarray}
$$</p>

<p>for any function $\psi$ whose expectation is defined.</p></li>
<li><p>If $h(\bar{g}\theta)$ depends on $\theta$ only through $h$, then there is a transformation $g^\ast$ such that
$h(\bar{g}\theta) = g^\ast h(\theta)$ for all $\theta\in\Omega$.</p></li>
<li><p>(Definition 2.4 in Chapter 3 TPE) A problem estimating $h(\theta)$ with the loss function $L$ is called <em>invariant</em> under $G$, if $L(\bar{g}\theta, g^\ast d) = L(\theta, d)$ and if $h(\bar{g}\theta)$ depends on $\theta$ only through $h$.</p></li>
</ul>

<h3>Equivariant estimators</h3>

<p>In an invariant estimation problem, an estimator $\delta(x)$ is <em>equivariant</em> if
$$\delta(gx) = g^\ast \delta(x),$$
for all $g\in G$ (Definition 2.5 in Chapter 3 TPE).
That is, the estimator $\delta$ respects the principle of invariance.</p>

<p>In particular, equation ($\ref{eq:invariant}$) implies that the risk function of any equivariant estimator is constant on <a href="https://en.wikipedia.org/wiki/Group_action#Orbits_and_stabilizers">orbits of the group of transformations $G$</a>.</p>

<h2>The least squares estimator is UMVU and MRE</h2>

<p>Consider a linear model $y = X\beta + \varepsilon$, where $y\in\mathbb{R}^n$, $X\in\mathbb{R}^{n\times p}$ with $p &lt; n$, $\mathrm{rank}(X) = p$, $\beta\in\mathbb{R}^p$, and $\varepsilon\subscript{i} \sim \mathrm{i.i.d.}\, N(0,\sigma^2)$ for all $i\in\{1,\dots,n\}$.</p>

<p>For convenience, denote $\xi := X\beta$. It holds that $\xi\in\Pi$, where $\Pi$ denotes a $p$-dimensional subspace of $\mathbb{R}^n$ (spanned by the columns of $X$).</p>

<h3>Orthogonal coordinate transformation</h3>

<p>Consider the orthogonal transformation $z = Qy$, where $Q\in\mathbb{R}^{n\times n}$ is an orthogonal matrix such that its first $p$ rows span $\Pi$. Denote $\eta := Q\xi$, the expectation of $z$. It follows that $\eta\subscript{p+1} = \dots = \eta\subscript{n} = 0$. Thus we have that
$z\subscript{i} \sim N(\eta\subscript{i}, \sigma^2)$ for $i=1,\dots,p$ and $z\subscript{j} \sim N(0,\sigma^2)$ for $j=p+1,\dots,n$. Moreover, all entries of $z$ are independent.</p>

<p>By writing the multivariate normal density of $z$ it becomes apparent that $z\subscript{1}, \dots, z\subscript{p}$ and $s^2 = \sum\subscript{j = p+1}^n z\subscript{j}^2$ are the complete and sufficient statistics for $(\eta, \sigma^2)$.</p>

<p>It follows that $\sum\subscript{i=1}^n \lambda\subscript{i} z\subscript{i}$ is UMVU for $\sum\subscript{i=1}^n \lambda\subscript{i} \eta\subscript{i}$ and $s^2 / (n-p)$ is UMVU for $\sigma^2$, because both estimators are unbiased and functions of complete and sufficient statistics.</p>

<p>Clearly, $\sum\subscript{i=1}^n \lambda\subscript{i} z\subscript{i}$ is equivariant under the transformations</p>

<p>$$
\begin{eqnarray}
z\subscript{i}^\prime &amp;=&amp; z\subscript{i} + a\subscript{i}, i = 1,\dots,p, \quad z\subscript{j}^\prime = z\subscript{j}, j = p+1,\dots,n, \nonumber \\\
\eta\subscript{i}^\prime &amp;=&amp; \eta\subscript{i} + a\subscript{i}, i = 1,\dots,p, \quad \eta\subscript{j}^\prime = \eta\subscript{j}, j = p+1,\dots,n, \nonumber \\\
d^\prime &amp;=&amp; d + \sum\subscript{i = 1}^p a\subscript{i} \lambda\subscript{i}. \nonumber
\end{eqnarray}
$$</p>

<p>It follows that the estimator $\sum\subscript{i=1}^n \lambda\subscript{i} z\subscript{i}$ is also the <em>minimum risk equivariant</em> (MRE) estimator of $\sum\subscript{i=1}^n \lambda\subscript{i} \eta\subscript{i}$ (with the loss function $L(\eta, d) = \rho(d - \sum \lambda\subscript{i} \eta\subscript{i})$, where $\rho$ is convex and even). Moreover, it can be shown that $s^2 / (n-p+2)$ is MRE for $\sigma^2$ under the loss function $(d-\sigma^2)^2 / \sigma^4$ (see problem 4.3 in Chapter 3 TPE).</p>

<p>We refer to Theorem 4.3 in Chapter 3 TPE and anything referenced to from therein for more rigour and detail.</p>

<h3>UMVU and MRE estimators in the original space</h3>

<p>We have shown the UMVU and MRE estimators in terms of $z$, the orthogonally transformed version of $y$. However, it would be more useful to have UMVU and MRE estimators in terms of the original variables $y$.</p>

<p>As is well-known, the <em>least squares estimator</em> of $\mathrm{E}(y) = \xi$ is given by $\hat{y} = X (X^T X)^{-1} X^T y$, which is an orthogonal projection of $y$ on $\Pi$. It can be found by minimizing the least squares $\|y - \xi\|\subscript{2}^2 = \|y - X\beta\|\subscript{2}^2$. We have that</p>

<p>$$
\begin{equation}
\label{eq:least_squares}
\sum\subscript{i=1}^n (y\subscript{i} - \xi\subscript{i})^2 = \sum\subscript{i=1}^p (z\subscript{i} - \eta\subscript{i})^2 + \sum\subscript{i=p+1}^n z\subscript{i}^2.
\end{equation}
$$</p>

<p>Since the left hand side is minimized by $\hat{y}$ and the right hand side is minimized by $\hat{\eta}\subscript{i} = z\subscript{i}$ for $i = 1,\dots,p$ (and $=0$ for $i&gt;p$), it holds that $\hat{y} = Q^T\hat{\eta}$. Thus, the LSE $\hat{y}$ is a linear function of $z$, and therefore the estimator $\sum\subscript{i=1}^n \lambda\subscript{i} \hat{y}\subscript{i}$ is UMVU for $\sum\subscript{i=1}^n \lambda\subscript{i} \xi\subscript{i}$ by the argumentation given above (namely because $\sum\subscript{i=1}^n \lambda\subscript{i} z\subscript{i}$ is UMVU for $\sum\subscript{i=1}^n \lambda\subscript{i} \eta\subscript{i}$).
For more detail see Chapter 3 Theorem 4.4 in TPE.</p>

<p>Likewise, it follows from the argumentation given above for the case of the orthogonal transform $z$ that the estimator $\sum\subscript{i=1}^n \lambda\subscript{i} \hat{y}\subscript{i}$ is MRE for $\sum\subscript{i=1}^n \lambda\subscript{i} \xi\subscript{i}$ under the transformation
$y^\prime = y + b$ with $b\in\Pi$ and with the loss function $L(\xi, d) = \rho(d - \sum \lambda\subscript{i} \xi\subscript{i})$ provided $\rho$ is convex and even.
See Chapter 3 Corollary 4.5 for detail.</p>

<p>Similarly, using the results given above for the orthogonal transform $z$, by reexpressing 
$s^2 = \sum\subscript{i=p+1}^n z\subscript{i}^2 = \sum\subscript{i=1}^n (y\subscript{i} - \hat{y}\subscript{i})^2$ (from equation ($\ref{eq:least_squares}$)) it follows that the UMVU and MRE estimators of $\sigma^2$ are given by
$\sum\subscript{i=1}^n (y\subscript{i} - \hat{y}\subscript{i})^2 / (n-p)$ and $\sum\subscript{i=1}^n (y\subscript{i} - \hat{y}\subscript{i})^2 / (n-p+2)$ respectively.</p>

<p>Finally, the LSE $\hat{\beta} = (X^T X)^{-1}X^T y$ is UMVU and MRE for $\beta$ by the above argumentation, because it can be written as a linear function of $\hat{y}$.</p>

<h2>The least squares estimator is BLUE</h2>

<p>A <em>best linear unbiased estimator</em> (BLUE) is an unbiased estimator that is linear in $y$ and achieves uniformly the smallest variance among all other linear unbiased estimators (i.e., UMVU among all linear estimators).</p>

<p>In the context of linear models, an advantage of this optimality criterion over the notions of UMVU and MRE is that it does not rely on the normality assumptions. That is, we merely need to assume that $\mathrm{E}(y) = \xi = X\beta$ and $\mathrm{Cov}(y) = \sigma^2 I$, without any further assumptions on the distribution.</p>

<p>Assume we aim to estimate $\sum\subscript{i=1}^n \lambda\subscript{i} \xi\subscript{i} = \lambda^T X\beta$. By linearity the estimator should have the form $\delta(y) = a^T y$. Unbiasedness implies that $a^T X \beta = \lambda^T X \beta$, from which it follows that $X^T (a - \lambda) \perp \beta$ for any $\beta\in\mathbb{R}^p$, and consequently $X^T a = X^T \lambda$. Taking $m\in\mathbb{R}^n$ to be a vector of Lagrange multipliers, the minimization problem becomes,</p>

<p>$$
\begin{eqnarray}
a + Xm &amp;=&amp; 0 \nonumber \\\
X^T a &amp;=&amp; X^T \lambda. \nonumber
\end{eqnarray}
$$</p>

<p>It is easily seen that this is solved by $a = X(X^T X)^{-1} X^T \lambda$ and $m = -(X^T X)^{-1} X^T \lambda$. In particular, $\hat{\beta} = (X^T X)^{-1} X^T y$ is BLUE for $\beta$.</p>

<p>TPE has a different approach of proving that LSE is BLUE (see Theorem 4.12 in Chapter 3, which TPE calls Gauss&#39; Theorem on Least Squares). Moreover, it follows that LSE is also MRE among all linear estimators (see Corollary 4.13 in Chapter 3 TPE).</p>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">If you find any mistake then please <a href="https://github.com/agisga/Lehmanns_TSH_and_TPE/issues">submit an issue in the corresponding github repository.</a></h2>

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>Some impressions from the books  "Testing Statistical Hypotheses" and "Theory of Point Estimation"</li>
          <li><a href="mailto:alexej.go@googlemail.com">alexej.go@googlemail.com</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/agisga">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">agisga</span>
            </a>
          </li>
          

        </ul>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text">A collection of random facts I observed while reading Lehmann's  "Testing Statistical Hypotheses" and "Theory of Point Estimation"
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
