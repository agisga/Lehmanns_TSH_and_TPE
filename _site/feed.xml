<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Some impressions from the books  &quot;Testing Statistical Hypotheses&quot; and &quot;Theory of Point Estimation&quot;</title>
    <description>A collection of random facts I observed while reading Lehmann&#39;s  &quot;Testing Statistical Hypotheses&quot; and &quot;Theory of Point Estimation&quot;
</description>
    <link>http://www.alexejgossmann.com//Lehmanns_TSH_and_TPE/</link>
    <atom:link href="http://www.alexejgossmann.com//Lehmanns_TSH_and_TPE/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Mon, 30 Nov 2015 17:27:05 -0600</pubDate>
    <lastBuildDate>Mon, 30 Nov 2015 17:27:05 -0600</lastBuildDate>
    <generator>Jekyll v2.4.0</generator>
    
      <item>
        <title>On the notion of unbiasedness of estimators, hypotheses tests, and confidence intervals</title>
        <description>&lt;p&gt;The following discusses various well-known definitions of unbiasedness, their generalizations and relationships with each other, as well as some of the underlying intuition (such as the relationship between hypotheses tests and confidence intervals).&lt;/p&gt;

&lt;h2&gt;Unbiased estimators&lt;/h2&gt;

&lt;p&gt;The well-known and widely used definition of an unbiased estimator $\hat{\theta}$ of a parameter $\theta$ is&lt;/p&gt;

&lt;p&gt;$$\mathrm{E}\subscript{\theta}(\hat{\theta}) = \theta.$$&lt;/p&gt;

&lt;p&gt;However it can be generalized as follows. Assume that there is a loss function $L(\theta, \hat{\theta})$, which only depends on the correct parameter $\theta$ and the estimate $\hat{\theta}$ (i.e. it measures how far off the estimator is from the parameter that it aims to estimate).
Then $\hat{\theta}$ is said to be unbiased for $\theta$ with respect to $L$, if for all $\theta^\prime$ it holds that&lt;/p&gt;

&lt;p&gt;$$\mathrm{E}\subscript{\theta}(L(\theta^\prime, \hat{\theta})) \geq \mathrm{E}\subscript{\theta}(L(\theta, \hat{\theta})).$$&lt;/p&gt;

&lt;p&gt;That is, if $\hat{\theta}$ is on average closer to the correct parameter $\theta$ than to any wrong parameter $\theta^\prime$ in the parameter space.&lt;/p&gt;

&lt;p&gt;When estimating a real valued $\theta$ with the square of the error as loss, the above condition becomes&lt;/p&gt;

&lt;p&gt;$$\mathrm{E}\subscript{\theta}\left(\left| \theta^\prime - \hat{\theta} \right|^2\right) \geq \mathrm{E}\subscript{\theta}\left(\left| \theta - \hat{\theta}\right|^2\right).$$&lt;/p&gt;

&lt;p&gt;If $\mathrm{E}\subscript{\theta}\hat{\theta}$ is one of the possible values of $\theta$, then by adding and subtracting $\mathrm{E}\subscript{\theta}\hat{\theta}$ inside the parentheses on both sides of the equation it follows that the above unbiasedness condition is satisfied if and only if&lt;/p&gt;

&lt;p&gt;$$\mathrm{E}\subscript{\theta}(\hat{\theta}) = \theta.$$&lt;/p&gt;

&lt;p&gt;This equivalence also holds under somewhat more general assumptions, see exercise 1.2 in TSH.&lt;/p&gt;

&lt;h2&gt;Unbiased tests&lt;/h2&gt;

&lt;p&gt;Consider a level $\alpha$ test $\phi$ of the hypothesis $H : \theta \in \Omega\subscript{H}$ against an alternative $K : \theta \in \Omega\subscript{K}$.
Denote the power function of $\phi$ by $\beta\subscript{\phi}(\theta) = \mathrm{E}\subscript{\theta} \phi(X)$.
Then it is natural to define unbiasedness of $\phi$ by the criterion&lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
\nonumber
\beta\subscript{\phi}(\theta) &amp;amp;\leq&amp;amp; \alpha \quad \mathrm{if}\, H : \theta \in \Omega\subscript{H}, \\\
\beta\subscript{\phi}(\theta) &amp;amp;\geq&amp;amp; \alpha \quad \mathrm{if}\,  K : \theta \in \Omega\subscript{K}. 
\nonumber
\end{eqnarray}
$$&lt;/p&gt;

&lt;p&gt;In particular, it follows that $\beta\subscript{\phi}(\theta) = \alpha$ on the common boundary of $\Omega\subscript{H}$ and $\Omega\subscript{K}$. In fact, a test that is the most powerful among all such tests, is UMP unbiased (Lemma 4.1.1 in TSH). &lt;/p&gt;

&lt;p&gt;However, the definition of an unbiased test can be generalized in the same way as that of an unbiased estimator shown above.
Assume that there is a loss function $L(\theta, \phi(x))$, which only depends on the true value of $\theta$ and the decision $\phi(x)$ takes by the test $\phi$. Then the hypothesis test is unbiased with respect to $L$, if for all $\theta^\prime$ it holds that&lt;/p&gt;

&lt;p&gt;$$\mathrm{E}\subscript{\theta}(L(\theta^\prime, \phi(X))) \geq \mathrm{E}\subscript{\theta}(L(\theta, \phi(X))).$$&lt;/p&gt;

&lt;p&gt;For the test $\phi$ of $H$ vs. $K$ let the loss function be equal to $\alpha$ if a Type II error is committed and equal $(1-\alpha)$ if a Type I error is committed. Then &lt;/p&gt;

&lt;p&gt;$$
\mathrm{E}\subscript{\theta}(L(\theta^\prime, \phi(X))) = 
\begin{cases}
\alpha (1 - \beta\subscript{\phi}(\theta)) \quad &amp;amp;\mathrm{if}&amp;amp;\, \theta^\prime \in \Omega\subscript{K}\\\ 
(1-\alpha) \beta\subscript{\phi}(\theta) \quad &amp;amp;\mathrm{if}&amp;amp;\, \theta^\prime \in \Omega\subscript{H},
\end{cases}
$$&lt;/p&gt;

&lt;p&gt;It follows that if $\theta \in \Omega\subscript{H}$ then $\alpha (1 - \beta\subscript{\phi}(\theta)) \geq (1-\alpha) \beta\subscript{\phi}(\theta)$, and consequently&lt;/p&gt;

&lt;p&gt;$$\beta\subscript{\phi}(\theta) \leq \alpha.$$&lt;/p&gt;

&lt;p&gt;Similarly, by considering $\theta\in\Omega\subscript{K}$, we get $\beta\subscript{\phi}(\theta) \geq \alpha$. Thus the usual definition is a special case of the more general loss-function-based definition.&lt;/p&gt;

&lt;h2&gt;Unbiased confidence sets&lt;/h2&gt;

&lt;p&gt;As is well-known, the defining condition for a confidence interval $\left(\underline{\theta}, \overline{\theta}\right)$ is&lt;/p&gt;

&lt;p&gt;$$P\subscript{\theta}\left(\underline{\theta}(X) \leq \theta \leq \overline{\theta}(X)\right) \geq 1-\alpha,$$&lt;/p&gt;

&lt;p&gt;for all $\theta$.&lt;/p&gt;

&lt;h3&gt;Hypotheses tests vs. confidence intervals&lt;/h3&gt;

&lt;p&gt;It is well-known that hypotheses tests and confidence intervals generally do exactly the same thing.
However, to describe with mathematical rigour in what sense it is true requires a little thinking.&lt;/p&gt;

&lt;p&gt;Consider a level $\alpha$ test of a two-sided hypothesis test $H : \theta = \theta\subscript{0}$ vs. $K : \theta \neq \theta\subscript{0}$, and denote its acceptance region by $A(\theta\subscript{0})$.
Define the inclusion region of the confidence set to be&lt;/p&gt;

&lt;p&gt;$$S(x) := \{ \theta : x\in A(\theta) \},$$&lt;/p&gt;

&lt;p&gt;that is, $\theta \in S(x)$ if and only if $x\in A(\theta)$. Then $S(x)$ defines a $(1-\alpha) \cdot 100\%$ confidence set, because for all $\theta$ we have&lt;/p&gt;

&lt;p&gt;$$P\subscript{\theta}(\theta \in S(x)) = P\subscript{\theta}(x\in A(\theta)) \geq 1 - \alpha.$$&lt;/p&gt;

&lt;p&gt;Conversely, if we start out with a family of confidence sets $\{S(x) : x\in\mathcal{X}\}$, and define $A(\theta) := \{x : \theta\in S(x)\}$, then for any $\theta$ it holds that&lt;/p&gt;

&lt;p&gt;$$P\subscript{\theta}(x\in A(\theta)) = P\subscript{\theta}(\theta \in S(x)) \geq 1 - \alpha.$$&lt;/p&gt;

&lt;p&gt;It follows that $P\subscript{\theta}(\mathrm{Type\,I\,error}) \leq \alpha$, that is, $A(\theta)$ is the acceptance region of a level $\alpha$ test.&lt;/p&gt;

&lt;h3&gt;Unbiased and uniformly most accurate unbiased confidence sets&lt;/h3&gt;

&lt;p&gt;Now it suggests itself to define an unbiased confidence set as one that stems from an unbiased hypothesis test by the above procedure. 
In the two-sided case discussed above this condition reduces to&lt;/p&gt;

&lt;p&gt;$$P\subscript{\theta}\left(\underline{\theta}(X) \leq \theta^\prime \leq \overline{\theta}(X)\right) \leq 1 - \alpha$$&lt;/p&gt;

&lt;p&gt;for all $\theta^\prime$ and $\theta$ such that $\theta \neq \theta^\prime$. That is, the inclusion probability of the null hypothesis parameter $\theta^\prime$ in the confidence interval, when the alternative $\theta$ is true, is less than the confidence level. Lemma 5.5.1 in TSH shows that the confidence set derived from an unbiased level $\alpha$ hypothesis test has indeed the form of an interval.&lt;/p&gt;

&lt;p&gt;Similarly, uniformly most accurate confidence intervals correspond to uniformly most powerful tests (see section 3.5 in TSH for more detail).
However, UMP tests usually do not exist, which is a reason to concentrate on unbiasedness instead. In particular, UMP unbiased tests correspond to uniformly most accurate unbiased confidence sets, i.e.  $S(x)$ such that for all $\theta^\prime$ and $\theta$ with $\theta\in K(\theta^\prime)$ the probability $P\subscript{\theta}(\theta^\prime\in S(x))$ is minimized.&lt;/p&gt;
</description>
        <pubDate>Fri, 30 Oct 2015 00:00:00 -0500</pubDate>
        <link>http://www.alexejgossmann.com//Lehmanns_TSH_and_TPE/unbiasedness/</link>
        <guid isPermaLink="true">http://www.alexejgossmann.com//Lehmanns_TSH_and_TPE/unbiasedness/</guid>
        
        
      </item>
    
      <item>
        <title>Conditional expectation, conditional distribution, sufficiency, decision procedures</title>
        <description>&lt;p&gt;Consider a random variable $X$ with sample space $(\mathcal{X}, \mathcal{A})$ and probability distribution $P^X$, and a statistic $T(X)$ with range space $(\mathcal{T}, \mathcal{B})$.&lt;/p&gt;

&lt;h3&gt;Definition [$\mathrm{E}(f(X)|t)$]&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Let $f(x)$ be a non-negative, $\mathcal{A}$-measurable and $P^X$-integrable function.
A $\mathcal{B}$-measurable function $g(t)$ is the conditional expectation of $X$ for given $t$, i.e. $\mathrm{E}(f(X)|t) = \mathrm{E}(f(X)|T=t) = g(t)$, if for all sets $B\in\mathcal{B}$ it holds that&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;$$\int\subscript{T^{-1}(B)} f(x) dP^X(x) = \int\subscript{B} g(t) dP^T(t).$$&lt;/p&gt;

&lt;p&gt;Some observations regarding this definition:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;In fact, if we define $f\subscript{0}(x) = g(T(x))$, then by Lemma 2.3.2 in TSH the above formula becomes&lt;/p&gt;

&lt;p&gt;$$\int\subscript{A} f(x) dP^X(x) = \int\subscript{A} f\subscript{0}(x) dP^X(x), \forall A \in \mathcal{A}\subscript{0},$$&lt;/p&gt;

&lt;p&gt;where $\mathcal{A}\subscript{0}$ is the $\sigma$-algebra induced by $T$.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The existence and uniqueness $(\mathcal{A}\subscript{0}, P^X)$ of such a function $f\subscript{0}$ follows from Radon-Nikodym Theorem.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If $f$ is not non-negative, then we can use the usual decomposition $f = f^+ - f^-$ and define&lt;/p&gt;

&lt;p&gt;$$\mathrm{E}(f(X)|t) = \mathrm{E}(f^+(X)|t) - \mathrm{E}(f^-(X)|t).$$&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Definition [$P(A|t)$]&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Let $I\subscript{A}(X)$ be a random variable that is equal to one if and only if $X\in A$. The conditional probability of $A$ given $T=t$ can be defined as&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;$$P(A|t) = E(I\subscript{A}(X) | t).$$&lt;/p&gt;

&lt;p&gt;This definition seems natural, and in fact, if $T$ has Euclidean domain and range spaces or if $\mathrm{E}|f(X)| &amp;lt; \infty$, then the above defines the &lt;em&gt;conditional probability distribution&lt;/em&gt; $P^{X|t}$ (see Theorems 2.5.2 and 2.5.3 in TSH).&lt;/p&gt;

&lt;h3&gt;Definition [Sufficiency]&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Let $\mathcal{P} = \{P\subscript{\theta} : \theta\in\Omega\}$ be a family of distributions over a sample space $(\mathcal{X}, \mathcal{A})$.&lt;/em&gt;
&lt;em&gt;$T$ is sufficient for $\mathcal{P}$ (or $\theta$) if $P\subscript{\theta}(A|t)$ is independent of $\theta$ for every $A\in\mathcal{A}$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In particular, the class of decision procedures depending on a sufficient statistic $T$ is &lt;em&gt;essentially complete&lt;/em&gt;. To see this, assume that the sample space is Euclidean, then by Theorem 2.5.1 in TSH there exists the conditional probability distribution $P^{X|t}$. Let $\phi(x)$ be a decision procedure. Given only the value of the sufficient statistic $T(X)$ (but not $X$), define another decision procedure $\psi(t)$ as a random sample from the distribution $P^{X|t}$. Then $\phi(X)$ and $\psi(T)$ have identical distributions. Consequently, both decision procedures have the same risk,&lt;/p&gt;

&lt;p&gt;$$R(\theta, \psi) = \mathrm{E}(L(\theta, \psi(T))) = \mathrm{E}(L(\theta, \phi(X))) = R(\theta, \phi).$$&lt;/p&gt;

&lt;p&gt;Thus, for any decision procedure that is based on $X$, there is a decision procedure based on $T$ that is equally good or better.&lt;/p&gt;

&lt;p&gt;For a proof in the general (non-Euclidean) case see exercise 2.13 in TSH.&lt;/p&gt;

&lt;h2&gt;General conditional expectation&lt;/h2&gt;

&lt;p&gt;Let $X$ and $Y$ be two real-valued random variables, which can be written as mappings $X: \Omega \to \mathbb{R}$ and $Y: \Omega \to \mathbb{R}$ over a measurable space $(\Omega, \mathcal{A}, P)$. The above definition of $\mathrm{E}(X|T(X)=t)$ suggests a similar definition of $\mathrm{E}(X|Y=y)$. Namely, $\mathrm{E}(X|Y=y) = g(y)$ if for all Borel sets $A$ it holds that&lt;/p&gt;

&lt;p&gt;$$\int\subscript{Y^{-1}(A)} X(\omega) dP(\omega) = \int\subscript{A} g(y) dP^Y(y).$$&lt;/p&gt;

&lt;p&gt;In fact, a more general version of this definition is given in Feller&amp;#39;s &amp;quot;An Introduction to Probability Theory and its Applications. Volume II&amp;quot; (10.6) as,&lt;/p&gt;

&lt;p&gt;$$\mathrm{E}(X\cdot I\subscript{A}(Y)) = \int\subscript{A} \mathrm{E}(X | y) \mu\{dy\},$$&lt;/p&gt;

&lt;p&gt;for any pair of random variables $X$ and $Y$.&lt;/p&gt;

&lt;p&gt;If $X$ and $Y$ are real-valued one-dimensional, then the pair $(X,Y)$ can be viewed as a random vector in the plane. Each set $\{Y \in A\}$ consists of parallels to the $x$-axis, and we can define a $\sigma$-algebra induced by $Y$ as the collection of all sets $\{Y \in A\}$, where $A$ are Borel sets. Then $\mathrm{E}(X|Y)$ is a random variable, such that $\mathrm{E}(X\cdot I\subscript{B}) = \mathrm{E}(\mathrm{E}(X|Y) \cdot I\subscript{B})$ for all $B=\{Y\in A\}$ with $A$ being a Borel set. This leads to the following general definition.&lt;/p&gt;

&lt;h3&gt;Definition [Conditional expectation]&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Let $\mathcal{A}$ be the underlying $\sigma$-algebra of sets, and let $\mathcal{B}$ be a $\sigma$-algebra contained in $\mathcal{A}$. Let $X$ be a random variable.&lt;/em&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;A random variable $U$ is called a conditional expectation of $X$ relative to $\mathcal{B}$ (or $U=\mathrm{E}(X|\mathcal{B})$), if it is $\mathcal{B}$-measurable and for all $B\in\mathcal{B}$ it holds that&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;$$\mathrm{E}(X\cdot I\subscript{B}) = \mathrm{E}(U \cdot I\subscript{B}).$$&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;If $\mathcal{B}$ is the $\sigma$-algebra generated by a random variable $Y$, then $\mathrm{E}(X|Y) = \mathrm{E}(X|\mathcal{B})$.&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Thu, 29 Oct 2015 00:00:00 -0500</pubDate>
        <link>http://www.alexejgossmann.com//Lehmanns_TSH_and_TPE/conditional_expectation/</link>
        <guid isPermaLink="true">http://www.alexejgossmann.com//Lehmanns_TSH_and_TPE/conditional_expectation/</guid>
        
        
      </item>
    
      <item>
        <title>An informal summary of Neyman-Pearson and generalizations</title>
        <description>&lt;p&gt;The following offers an informal view on the fundamental lemma of Neyman and Pearson and generalizations thereof.
For a mathematically rigorous presentation see the corresponding results in TSH, which are cited in this article.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Notation&lt;/em&gt;: MP = &amp;quot;most powerful&amp;quot;, UMP = &amp;quot;uniformly most powerful&amp;quot;, $H$ denotes the null hypothesis, $K$ denotes the alternative hypothesis, $\alpha$ denotes the level of the hypothesis test, lower case Roman letters denote realizations of random variables (upper case).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;[Simple hypotheses]&lt;/em&gt; What is actually called fundamental lemma of Neyman and Pearson in TSH (Theorem 3.2.1) is concerned with a test of two simple hypotheses. Under the null hypothesis the random variable $X$ is assumed to follow a probability distribution with density $p\subscript{0}$, while under the alternative hypothesis the density is $p\subscript{1}$. &lt;/p&gt;

&lt;p&gt;Consider $H : p\subscript{0}$ vs. $K : p\subscript{1}$. MP test $\phi$ exists. It rejects the null if $\frac{p\subscript{1}}{p\subscript{0}} &amp;gt; k$, accepts the null if $\frac{p\subscript{1}}{p\subscript{0}} &amp;lt; k$, and rejects with probability $\gamma$ if $\frac{p\subscript{1}}{p\subscript{0}} = k$, where $\gamma$ and $k$ are chosen to satisfy $\mathrm{E}\subscript{p\subscript{0}} \phi(X) = \alpha$.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;[Monotone likelihood ratio, one-sided, one-param. exp. fam.]&lt;/em&gt; For one-parameter families of distributions and one-sided hypotheses, the Neyman-Pearson lemma can be generalized to construct a UMP test if the distributions in question have monotone likelihood ratios. This is Theorem 3.4.1 in TSH.&lt;/p&gt;

&lt;p&gt;Consider $H : \theta \leq \theta\subscript{0}$ vs. $K : \theta &amp;gt; \theta\subscript{0}$ ($\theta \in \mathbb{R}$). If $\frac{p\subscript{\theta^\prime}(x)}{p\subscript{\theta}(x)}$ is nondecreasing in $T(x)$ for any $\theta &amp;lt; \theta^\prime$, then a UMP test $\phi$ exists. It rejects if $T(x) &amp;gt; C$, accepts if $T(x) &amp;lt; C$, and rejects with probability $\gamma$ if $T(x) = C$, where $C$ and $\gamma$ are determined by $\mathrm{E}\subscript{\theta\subscript{0}} \phi(X) = \alpha$.&lt;/p&gt;

&lt;p&gt;By interchanging the inequalities one obtains a UMP test for the dual problem $H : \theta \geq \theta\subscript{0}$ vs. $K : \theta &amp;lt; \theta\subscript{0}$.&lt;/p&gt;

&lt;p&gt;Additionally, this test minimizes the Type I error subject to $\mathrm{E}\subscript{\theta\subscript{0}} \phi(X) = \alpha$.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;[Two-sided null in one-param. exp. fam.]&lt;/em&gt; An analogous UMP test exists for a two-sided null hypothesis $H : \theta \leq \theta\subscript{1} \,\mathrm{or}\, \theta \geq \theta\subscript{2}$ in one-parameter exponential families. It rejects if $C\subscript{1} &amp;lt; T(x) &amp;lt; C\subscript{2}$, accepts if $T(x) &amp;lt; C\subscript{1}$ or $T(x) &amp;gt; C\subscript{2}$, rejects with probability $\gamma$ if $T(x) = C\subscript{i}$ (for $i=1$ or $i=2$), and satisfies $\mathrm{E}\subscript{\theta\subscript{1}} \phi(X) = \alpha = \mathrm{E}\subscript{\theta\subscript{2}} \phi(X)$. Subject to the last condition, this test minimizes the Type I error. See Theorem 3.7.1 in TSH.&lt;/p&gt;

&lt;p&gt;A UMP test for a two-sided alternative hypothesis $K : \theta \leq \theta\subscript{1} \,\mathrm{or}\, \theta \geq \theta\subscript{2}$ does not exist (&lt;a href=&quot;/Lehmanns_TSH_and_TPE/two-sided_hypotheses/&quot;&gt;e.g. see my corresponding write-up&lt;/a&gt;). However, a UMP &lt;em&gt;unbiased&lt;/em&gt; test analogous to the above exists (see Section 4.2 in TSH).&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;[UMP unbiased tests, multi-param. exp. fam.]&lt;/em&gt; For multi-parameter exponential families the existence of a UMP test typically cannot be established. However, UMP &lt;em&gt;unbiased&lt;/em&gt; tests can be constructed without great difficulties. Assume that $\theta\in\mathbb{R}$ is the parameter to be tested, and that $(U, T)$ is a sufficient statistic, where $U$ corresponds to $\theta$ and $T$ corresponds to all other parameters. Then UMP unbiased tests exist for most of the usual hypotheses, and can be written in the same way as in the one-parameter case, except that now all constants specifying the rejection region depend on $T$ (e.g. the rejection rule has the form $u &amp;gt; C(t)$, etc.). Also, the size of the test is measured conditional on $T$.&lt;/p&gt;

&lt;p&gt;See Theorem 4.4.1 in TSH.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;[UMP unbiased and independent of sufficient statistic]&lt;/em&gt; UMP unbiased tests for multi-parameter exponential families, as discussed in the last point, are independent of $T$ if a number of additional conditions are satisfied. For example, assume that $V = h(U, T)$ is independent of $T$ (with $\theta = \theta\subscript{1}$ and $\theta = \theta\subscript{2}$) and that $h$ is increasing in $u$. Then a UMP unbiased test for a two-sided null hypothesis rejects if $C\subscript{1} &amp;lt; v &amp;lt; C\subscript{2}$, accepts if $v &amp;lt; C\subscript{1}$ or $v &amp;gt; C\subscript{2}$, etc.&lt;/p&gt;

&lt;p&gt;See Theorem 5.1.1 in TSH for more.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;[UMP invariant tests]&lt;/em&gt; If the problem of testing $H : \Omega\subscript{0}$ vs. $K : \Omega\subscript{1}$ remains invariant under a finite group $G = \{g\subscript{1}, g\subscript{2}, \dots, g\subscript{N} \}$, then there exists a UMP invariant test that rejects when $\frac{\sum p\subscript{\overline{g}\subscript{i} \theta\subscript{1}} (x)}{\sum p\subscript{\overline{g}\subscript{i} \theta\subscript{0}} (x)} &amp;gt; C$ (for any $\theta\subscript{0} \in \Omega\subscript{0}$ and any $\theta\subscript{1}$ in $\Omega\subscript{1}$). See Theorem 6.3.1 in TSH.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;[Finite composite null]&lt;/em&gt; When the null hypothesis specifies that $X$ is distributed according to one of finitely many densities $p\subscript{1}, p\subscript{2}, \dots, p\subscript{m}$, and the alternative hypothesis is $p\subscript{m+1}$, then there exists a test $\phi$ that maximizes $\int \phi p\subscript{m+1} d\mu$. For suitable constants $k\subscript{1}, k\subscript{2}, \dots, k\subscript{m}$, this test rejects the null if $p\subscript{m+1}(x) &amp;gt; \sum\subscript{i=1}^m k\subscript{i} p\subscript{i}(x)$, it accepts the null if $p\subscript{m+1}(x) &amp;lt; \sum\subscript{i=1}^m k\subscript{i} p\subscript{i}(x)$, and it satisfies $\int \phi p\subscript{i} d\mu \leq \alpha$ for $i = 1,2,\dots,m$.&lt;/p&gt;

&lt;p&gt;See Theorem 3.6.1 and Corollary 3.6.1 in TSH for more detail.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;[Least favorable distributions]&lt;/em&gt;  Assume a setting similar to the one in the last point, except that the number of distributions under the null hypothesis does not need to be finite. That is, $H : f\subscript{\theta}, \theta \in \omega$ vs. $K : g$.
One can define a &lt;em&gt;least favorable&lt;/em&gt; distribution $\Lambda$ over $\omega$ and assume that $\theta \sim \Lambda$. As $\Lambda$ is least favorable, one can expect that it leads to a hypothesis test that works best in the worst case (i.e. at values $\theta$ closest to $K$). Thus, $\Lambda$ will typically be a distribution of $H$ that is closest to $K$. In particular, one would have $\Lambda(\omega^\prime) = 1$ for some &amp;quot;boundary region&amp;quot; $\omega^\prime$ of $\omega$. Then a MP test $\phi$ exists. It rejects if $g(x) &amp;gt; k \int f\subscript{\theta}(x) d\Lambda(\theta)$, accepts if $g(x) &amp;lt; k \int f\subscript{\theta}(x) d\Lambda(\theta)$, and satisfies $\sup\subscript{\theta\in\omega} \mathrm{E}\subscript{\theta} \phi(X) = \alpha$.&lt;/p&gt;

&lt;p&gt;See Theorem 3.8.1 and Corollary 3.8.1 in TSH for rigour and detail.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;[Maximin tests]&lt;/em&gt; The same approach can also be generalized to test $H : f\subscript{\theta}, \theta \in \omega$ vs. $K : f\subscript{\theta}, \theta \in \omega^\prime$. However, one has to ditch the UMP condition in favor of the condition that $\inf\subscript{\theta\in\omega^\prime} \mathrm{E}\subscript{\theta} \phi(X)$ is maximized under the constraint $\sup\subscript{\theta\in\omega} \mathrm{E}\subscript{\theta} \phi(X) \leq \alpha$. Such a test is called a &lt;em&gt;maximin&lt;/em&gt; test, and is established in Theorem 8.1.1 and Corollary 8.1.1 in TSH. It rejects if $\int\subscript{\omega^\prime} f\subscript{\theta}(x) d\Lambda^\prime(\theta)(x) &amp;gt; C \int\subscript{\omega} f\subscript{\theta}(x) d\Lambda(\theta)$, for suitably chosen distributions $\Lambda^\prime$ and $\Lambda$.&lt;/p&gt;

&lt;p&gt;Further, Theorem 8.5.1 (Hunt-Stein) and Lemma 8.4.1 in TSH establish the existence of almost invariant tests satisfying the maximin property.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 28 Oct 2015 00:00:00 -0500</pubDate>
        <link>http://www.alexejgossmann.com//Lehmanns_TSH_and_TPE/neyman_pearson/</link>
        <guid isPermaLink="true">http://www.alexejgossmann.com//Lehmanns_TSH_and_TPE/neyman_pearson/</guid>
        
        
      </item>
    
      <item>
        <title>Permutation tests</title>
        <description>&lt;p&gt;When a parametric probabilistic model cannot be assumed, one can still construct exact level-$\alpha$ hypotheses tests as permutation tests. Here, based on sections 5.8 and 5.9 of TSH, I discuss the concept by considering as an example a permutation test for the difference of two means. &lt;/p&gt;

&lt;p&gt;Assume that each of the random variables $X\subscript{1}, \dots, X\subscript{m}$ has mean $\eta$ and that each of $Y\subscript{1}, ..., Y\subscript{n}$ has mean $\xi$. Additionally assume that the distributions of all those variables differ only with respect to the mean, for example, $X\subscript{i} \sim \mathrm{i.i.d.}\, f(x\subscript{i})$ and $Y\subscript{i} \sim \mathrm{i.i.d.}\, f(y\subscript{i} - \Delta)$ with $\Delta = \eta - \xi$. The density function $f$ is not known apart from the fact that it is continuous a.e. We want to test the hypothesis $H : \Delta = 0$.&lt;/p&gt;

&lt;p&gt;Let $N:=n+m$, denote the random vector containing all $X$s and $Y$s as $Z := (X^T, Y^T)^T$, and let $S(z)$ be the set of all permutations of the entries of a realization $z$ of the random vector $Z$.
Then a level-$\alpha$ test $\phi$ has to satisfy&lt;/p&gt;

&lt;p&gt;$$\int \phi(z) \prod\subscript{i=1}^N f(z\subscript{i}) dz = \alpha.$$&lt;/p&gt;

&lt;p&gt;Interestingly, it turns out that this equality holds if and only if&lt;/p&gt;

&lt;p&gt;$$\frac{1}{N!} \sum\subscript{w\in S(z)} \phi(w) = \alpha.$$&lt;/p&gt;

&lt;p&gt;A more general result that accounts for population stratification is given by theorem 5.8.1 in TSH.&lt;/p&gt;

&lt;p&gt;The power of $\phi$  against an alternative $h(z)$ is given by&lt;/p&gt;

&lt;p&gt;$$\int \phi(z) h(z) dz = \int \mathrm{E}\left(\phi(Z) \middle| T=t\right) dP^T(t).$$&lt;/p&gt;

&lt;p&gt;Let $T(Z)$ be the order statistic. It holds that $S(z) = S(T(z)) = S(t)$, and from the expression of the conditional expectation $\mathrm{E}\left(\phi(Z) \middle| T=t\right)$ (see Example 2.4.1 and Problem 2.6), it can be further derived that the most powerful test $\phi$ maximizes&lt;/p&gt;

&lt;p&gt;$$\sum\subscript{z\in S(t)} \phi(z) \frac{h(z)}{\sum\subscript{w\in S(z)} h(w)}$$&lt;/p&gt;

&lt;p&gt;subject to&lt;/p&gt;

&lt;p&gt;$$\frac{1}{N!} \sum\subscript{z\in S(t)} \phi(z) = \alpha.$$&lt;/p&gt;

&lt;p&gt;Now, the Neyman-Pearson fundamental lemma implies that the hypothesis should be rejected whenever $\frac{h(z)N!}{\sum\subscript{w\in S(z)} h(w)}$ is too large.
This leads to a most powerful test $\phi$ given by&lt;/p&gt;

&lt;p&gt;$$\phi(z) = \begin{cases}
1, \quad\mathrm{if}\, h(z) &amp;gt; C(T(z)), \\\
\gamma, \quad\mathrm{if}\, h(z) = C(T(z)), \\\
0, \quad\mathrm{if}\, h(z) &amp;lt; C(T(z)).
\end{cases}$$&lt;/p&gt;

&lt;p&gt;Thus the test is carried out by... &lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;ordering the points in $S(z)$ in a decreasing order according to $h$,&lt;/li&gt;
&lt;li&gt;rejecting if $h(z)$ is one of the $k$ largest values and rejecting with probability $\gamma$ if $h(z)$ is $(k+1)$st largest, where $k$ and $\gamma$ are determined by&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;$$k+\gamma = \alpha \cdot N!$$&lt;/p&gt;

&lt;p&gt;More general versions of this approach, which incorporate population stratification and randomization, are given in section 5.8-5.13 in TSH.&lt;/p&gt;

&lt;p&gt;The above test is not UMP because it depends on $h$. However, it can be shown that if under the null hypothesis each $Z\subscript{i}$ follows the same normal distribution $\mathcal{N}(\xi, \sigma^2)$, then the derived test is most powerful among all unbiased tests of level $\alpha$ against all normal alternatives under consideration (see Lemma 5.9.1 in TSH for an even more general result).
Such an approach is appropriate when the data is assumed to be approximately normal but the assumption is not considered reliable. The permutation test is maximizing the power against all normal alternatives, while still being unbiased against all other alternatives.&lt;/p&gt;
</description>
        <pubDate>Tue, 27 Oct 2015 00:00:00 -0500</pubDate>
        <link>http://www.alexejgossmann.com//Lehmanns_TSH_and_TPE/permutation_tests/</link>
        <guid isPermaLink="true">http://www.alexejgossmann.com//Lehmanns_TSH_and_TPE/permutation_tests/</guid>
        
        
      </item>
    
      <item>
        <title>UMP tests for two-sided hypotheses</title>
        <description>&lt;p&gt;The (non-) existence of uniformly most powerful (or UMP) tests for two-sided hypotheses is an interesting phenomenon. &lt;/p&gt;

&lt;h3&gt;Example of existence&lt;/h3&gt;

&lt;p&gt;First let&amp;#39;s look at an example when such a test does exist. This is Problem 3.2 in TSH.&lt;/p&gt;

&lt;p&gt;For $i = 1,\dots, n$ let $X\subscript{i}$ be i.i.d. $\mathrm{Uniform}(0,\theta)$ random variables, denote their realizations by lower case $x\subscript{i}$s, and let $X$ denote the vector of the $X\subscript{i}$s. Consider the hypothesis $H : \theta = \theta\subscript{0}$ against the alternative $K : \theta \neq \theta\subscript{0}$.&lt;/p&gt;

&lt;p&gt;Denote $x\subscript{(n)} := \max\{x\subscript{1}, \dots, x\subscript{n}\}$. Let $\phi$ be a hypothesis test which rejects $H : \theta = \theta\subscript{0}$ in favor of a two-sided alternative, if either $x\subscript{(n)} \geq \theta\subscript{0}$ or $x\subscript{(n)} &amp;lt; \theta\subscript{0} \sqrt[n]{\alpha}$.&lt;/p&gt;

&lt;h4&gt;Proof&lt;/h4&gt;

&lt;p&gt;Using the fundamental lemma of Neyman and Pearson, it is straightforward to prove that $\phi$ is UMP. Namely, $\phi$ is a UMP test at level $\alpha$ by Neyman-Pearson, if for any fixed $\theta\subscript{1} \neq \theta\subscript{0}$, the test $\phi$ can be written as &lt;/p&gt;

&lt;p&gt;$$\phi(x) = \begin{cases} 
1, \quad &amp;amp;\mathrm{if}\, p\subscript{\theta\subscript{1}}(x) &amp;gt; k p\subscript{\theta\subscript{0}}(x),\\\
0, \quad &amp;amp;\mathrm{if}\, p\subscript{\theta\subscript{1}}(x) &amp;lt; k p\subscript{\theta\subscript{0}}(x), 
\end{cases}$$&lt;/p&gt;

&lt;p&gt;with a suitable $k$, and if it satisfies&lt;/p&gt;

&lt;p&gt;$$\mathrm{E}\subscript{\theta\subscript{0}} \phi(X) = \alpha.$$&lt;/p&gt;

&lt;p&gt;We have that&lt;/p&gt;

&lt;p&gt;$$\begin{eqnarray}
\nonumber
\mathrm{E}\subscript{\theta\subscript{0}} \phi(X) &amp;amp;=&amp;amp; P\subscript{\theta\subscript{0}}\left(X\subscript{(n)} &amp;gt; \theta\subscript{0}\right) + P\subscript{\theta\subscript{0}}\left(X\subscript{(n)} &amp;lt; \theta\subscript{0}\sqrt[n]{\alpha}\right)\\\
&amp;amp;=&amp;amp; 0 + \left(\frac{\theta\subscript{0} \sqrt[n]{\alpha}}{\theta\subscript{0}}\right)^n = \alpha.
\nonumber
\end{eqnarray}$$&lt;/p&gt;

&lt;p&gt;As for the other Neyman-Pearson condition, we have to consider multiple cases:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If $\theta\subscript{1} &amp;gt; \theta\subscript{0}$, then $k = \left(\frac{\theta\subscript{0}}{\theta\subscript{1}}\right)^n$ yields the desired result.&lt;/li&gt;
&lt;li&gt;If $\theta\subscript{0}\sqrt[n]{\alpha} &amp;lt; \theta\subscript{1} &amp;lt; \theta\subscript{0}$, then $k = \left(\frac{\theta\subscript{0}}{\theta\subscript{1}}\right)^n$ can be used as well.&lt;/li&gt;
&lt;li&gt;If $\theta\subscript{1} &amp;lt; \theta\subscript{0}\sqrt[n]{\alpha} &amp;lt; \theta\subscript{0}$, then $k = 0$.&lt;/li&gt;
&lt;/ul&gt;

&lt;div align=&quot;right&quot;&gt;
$\blacksquare$
&lt;/div&gt;

&lt;h3&gt;Example of non-existence&lt;/h3&gt;

&lt;p&gt;Thus, we saw an example of a UMP test for a two-sided hypothesis. &lt;/p&gt;

&lt;p&gt;However, when the underlying distribution comes from an exponential family, then a UMP test does not exist for $H : \theta = \theta\subscript{0}$ vs. $K : \theta \neq \theta\subscript{0}$ (Problem 3.54 in TSH). This follows quite easily from the consideration of UMP tests for the one-sided hypotheses $H\subscript{1} : \theta \leq \theta\subscript{0}$ vs. $K\subscript{1} : \theta &amp;gt; \theta\subscript{0}$, and $H\subscript{2} : \theta \geq \theta\subscript{0}$ vs. $K\subscript{2} : \theta &amp;lt; \theta\subscript{0}$.
A detailed proof follows.&lt;/p&gt;

&lt;h4&gt;Proof&lt;/h4&gt;

&lt;p&gt;According to Theorem 3.4.1 in TSH, a UMP test of $H\subscript{1}$ exists and can be written as&lt;/p&gt;

&lt;p&gt;$$\phi\subscript{1}(x) = \begin{cases} 
1, \quad &amp;amp;\mathrm{if}\, T(x) &amp;gt; C\subscript{1},\\\
0, \quad &amp;amp;\mathrm{if}\, T(x) &amp;lt; C\subscript{1}.
\end{cases}$$&lt;/p&gt;

&lt;p&gt;Similarly, a UMP test of $H\subscript{2}$ exists and can be written as&lt;/p&gt;

&lt;p&gt;$$\phi\subscript{2}(x) = \begin{cases} 
1, \quad &amp;amp;\mathrm{if}\, T(x) &amp;lt; C\subscript{2},\\\
0, \quad &amp;amp;\mathrm{if}\, T(x) &amp;gt; C\subscript{2}.
\end{cases}$$&lt;/p&gt;

&lt;p&gt;Clearly, $\phi\subscript{1}$ and $\phi\subscript{2}$ are level-$\alpha$ tests for $H$ vs. $K$ as well.&lt;/p&gt;

&lt;p&gt;Let $\phi\subscript{0}$ be a level-$\alpha$ test of $H$ vs. $K$. Fix a $\theta\subscript{1} &amp;gt; \theta\subscript{0}$ and a $\theta\subscript{2} &amp;lt; \theta\subscript{0}$. Assume that &lt;/p&gt;

&lt;p&gt;$$\mathrm{E}\subscript{\theta\subscript{i}} \phi\subscript{0}(X) \geq \mathrm{E}\subscript{\theta\subscript{i}} \phi\subscript{i}(X)$$&lt;/p&gt;

&lt;p&gt;for $i = 1,2$. Then $\phi\subscript{0}$ is most powerful for testing $\theta\subscript{0}$ vs. $\theta\subscript{1}$ and for testing $\theta\subscript{0}$ vs. $\theta\subscript{2}$. Thus, by the fundamental lemma of Neyman and Pearson the UMP test can be rewritten as&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\phi\subscript{0}(x) = \begin{cases} 
1, \quad &amp;amp;\mathrm{if}\, p\subscript{\theta\subscript{1}}(x) &amp;gt; k\subscript{1} p\subscript{\theta\subscript{0}}(x),\\\
0, \quad &amp;amp;\mathrm{if}\, p\subscript{\theta\subscript{1}}(x) &amp;lt; k\subscript{1} p\subscript{\theta\subscript{0}}(x), 
\end{cases}
\label{eq1}
\end{equation}
$$&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\phi\subscript{0}(x) = \begin{cases} 
1, \quad &amp;amp;\mathrm{if}\, p\subscript{\theta\subscript{2}}(x) &amp;gt; k\subscript{2} p\subscript{\theta\subscript{0}}(x),\\\
0, \quad &amp;amp;\mathrm{if}\, p\subscript{\theta\subscript{2}}(x) &amp;lt; k\subscript{2} p\subscript{\theta\subscript{0}}(x).
\end{cases}
\label{eq2}
\end{equation}
$$&lt;/p&gt;

&lt;p&gt;Let $x$ be such that $\phi\subscript{0}(x) = 1$. Now, from the monotonicity of the likelihood ratio, it follows that&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;if $T(y) &amp;gt; T(x)$ then $\phi\subscript{0}(y) = 1$ (by equation $\eqref{eq1}$),&lt;/li&gt;
&lt;li&gt;if $T(y) &amp;lt; T(x)$ then $\phi\subscript{0}(y) = 1$ (by equation $\eqref{eq2}$).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That is, either $\phi\subscript{0}(y) = 1$ for all $y$ or $\phi\subscript{0}(x) \neq 1$ for all $x$. A contradiction. It follows that $\phi\subscript{0}$ can not be more powerful than $\phi\subscript{1}$ for testing $\theta\subscript{0}$ vs. $\theta\subscript{1}$ and than $\phi\subscript{2}$ for testing $\theta\subscript{0}$ vs. $\theta\subscript{2}$. Thus, a UMP test for $H$ vs. $K$ does not exist.&lt;/p&gt;

&lt;div align=&quot;right&quot;&gt;
$\blacksquare$
&lt;/div&gt;

&lt;p&gt;Even though a UMP test for the two-sided hypothesis considered above does not exist, there exist a UMP unbiased test (i.e. a test that is uniformly most powerful among all unbiased tests). For detail see Section 4.2 in TSH.&lt;/p&gt;
</description>
        <pubDate>Mon, 26 Oct 2015 00:00:00 -0500</pubDate>
        <link>http://www.alexejgossmann.com//Lehmanns_TSH_and_TPE/two-sided_hypotheses/</link>
        <guid isPermaLink="true">http://www.alexejgossmann.com//Lehmanns_TSH_and_TPE/two-sided_hypotheses/</guid>
        
        
      </item>
    
      <item>
        <title>Least squares estimators are nice! PART 1 (UMVU, MRE, BLUE)</title>
        <description>&lt;p&gt;The well-known least squares estimator (LSE) for the coefficients of a linear model is the &amp;quot;best&amp;quot; possible estimator according to several different criteria. Three types of such optimality conditions under which the LSE is &amp;quot;best&amp;quot; are discussed below. In the process, we also briefly look at the &amp;quot;best&amp;quot; estimators of the variance in a linear model.&lt;/p&gt;

&lt;p&gt;Let&amp;#39;s fix the concepts first, and then explore how they apply to LSE.&lt;/p&gt;

&lt;h2&gt;Some definitions and implications&lt;/h2&gt;

&lt;h3&gt;UMVU estimators&lt;/h3&gt;

&lt;p&gt;As one would expect, a &lt;em&gt;uniform minimum variance unbiased&lt;/em&gt; (or UMVU) estimator $\delta(x)$ of $g(\theta)$ is an unbiased estimator such that
$\mathrm{Var}\subscript{\theta} \delta(X) \leq \mathrm{Var}\subscript{\theta} \delta^\prime(X)$ for any other unbiased estimator 
$\delta^\prime(x)$ of $g(\theta)$ and any $\theta\in\Omega$.&lt;/p&gt;

&lt;h3&gt;Invariance&lt;/h3&gt;

&lt;p&gt;Let $X \sim P\subscript{\theta}$ for some $\theta\in\Omega$. That is, $X$ is distributed according to one of the distributions in the family $\mathcal{P} = \{ P\subscript{\theta}, \theta \in \Omega \}$ of distributions. Let $G$ be the group generated by the set of all bijective transformations of the sample space of $X$ onto itself.&lt;/p&gt;

&lt;p&gt;If for any $g\in G$ it holds that $gX \sim P\subscript{\theta^\prime}$ for some $\theta^\prime \in \Omega$, and if as $\theta$ traverses $\Omega$ so does $\theta^\prime$, then $\mathcal{P}$ is &lt;em&gt;invariant&lt;/em&gt; under $G$ (Definition 2.1 in Chapter 3 TPE).&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The principle of invariance has some interesting implications:&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;If $G$ leaves $\mathcal{P}$ invariant, then there must be a bijective transformation $\bar{g}$ such that $\theta^\prime = \bar{g}\theta$. Such transformations $\bar{g}$ form a group $\overline{G}$, and we have that&lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
\nonumber
P\subscript{\theta}(gX \in A) &amp;amp;=&amp;amp; P\subscript{\bar{g}\theta}(X \in A) \\\
E\subscript{\theta} \psi(gX) &amp;amp;=&amp;amp; E\subscript{\bar{g}\theta} \psi(X),
\label{eq:invariant}
\end{eqnarray}
$$&lt;/p&gt;

&lt;p&gt;for any function $\psi$ whose expectation is defined.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;If $h(\bar{g}\theta)$ depends on $\theta$ only through $h$, then there is a transformation $g^\ast$ such that
$h(\bar{g}\theta) = g^\ast h(\theta)$ for all $\theta\in\Omega$.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;(Definition 2.4 in Chapter 3 TPE) A problem estimating $h(\theta)$ with the loss function $L$ is called &lt;em&gt;invariant&lt;/em&gt; under $G$, if $L(\bar{g}\theta, g^\ast d) = L(\theta, d)$ and if $h(\bar{g}\theta)$ depends on $\theta$ only through $h$.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Equivariant estimators&lt;/h3&gt;

&lt;p&gt;In an invariant estimation problem, an estimator $\delta(x)$ is &lt;em&gt;equivariant&lt;/em&gt; if
$$\delta(gx) = g^\ast \delta(x),$$
for all $g\in G$ (Definition 2.5 in Chapter 3 TPE).
That is, the estimator $\delta$ respects the principle of invariance.&lt;/p&gt;

&lt;p&gt;In particular, equation ($\ref{eq:invariant}$) implies that the risk function of any equivariant estimator is constant on &lt;a href=&quot;https://en.wikipedia.org/wiki/Group_action#Orbits_and_stabilizers&quot;&gt;orbits of the group of transformations $G$&lt;/a&gt;.&lt;/p&gt;

&lt;h2&gt;The least squares estimator is UMVU and MRE&lt;/h2&gt;

&lt;p&gt;Consider a linear model $y = X\beta + \varepsilon$, where $y\in\mathbb{R}^n$, $X\in\mathbb{R}^{n\times p}$ with $p &amp;lt; n$, $\mathrm{rank}(X) = p$, $\beta\in\mathbb{R}^p$, and $\varepsilon\subscript{i} \sim \mathrm{i.i.d.}\, N(0,\sigma^2)$ for all $i\in\{1,\dots,n\}$.&lt;/p&gt;

&lt;p&gt;For convenience, denote $\xi := X\beta$. It holds that $\xi\in\Pi$, where $\Pi$ denotes a $p$-dimensional subspace of $\mathbb{R}^n$ (spanned by the columns of $X$).&lt;/p&gt;

&lt;h3&gt;Orthogonal coordinate transformation&lt;/h3&gt;

&lt;p&gt;Consider the orthogonal transformation $z = Qy$, where $Q\in\mathbb{R}^{n\times n}$ is an orthogonal matrix such that its first $p$ rows span $\Pi$. Denote $\eta := Q\xi$, the expectation of $z$. It follows that $\eta\subscript{p+1} = \dots = \eta\subscript{n} = 0$. Thus we have that
$z\subscript{i} \sim N(\eta\subscript{i}, \sigma^2)$ for $i=1,\dots,p$ and $z\subscript{j} \sim N(0,\sigma^2)$ for $j=p+1,\dots,n$. Moreover, all entries of $z$ are independent.&lt;/p&gt;

&lt;p&gt;By writing the multivariate normal density of $z$ it becomes apparent that $z\subscript{1}, \dots, z\subscript{p}$ and $s^2 = \sum\subscript{j = p+1}^n z\subscript{j}^2$ are the complete and sufficient statistics for $(\eta, \sigma^2)$.&lt;/p&gt;

&lt;p&gt;It follows that $\sum\subscript{i=1}^n \lambda\subscript{i} z\subscript{i}$ is UMVU for $\sum\subscript{i=1}^n \lambda\subscript{i} \eta\subscript{i}$ and $s^2 / (n-p)$ is UMVU for $\sigma^2$, because both estimators are unbiased and functions of complete and sufficient statistics.&lt;/p&gt;

&lt;p&gt;Clearly, $\sum\subscript{i=1}^n \lambda\subscript{i} z\subscript{i}$ is equivariant under the transformations&lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
z\subscript{i}^\prime &amp;amp;=&amp;amp; z\subscript{i} + a\subscript{i}, i = 1,\dots,p, \quad z\subscript{j}^\prime = z\subscript{j}, j = p+1,\dots,n, \nonumber \\\
\eta\subscript{i}^\prime &amp;amp;=&amp;amp; \eta\subscript{i} + a\subscript{i}, i = 1,\dots,p, \quad \eta\subscript{j}^\prime = \eta\subscript{j}, j = p+1,\dots,n, \nonumber \\\
d^\prime &amp;amp;=&amp;amp; d + \sum\subscript{i = 1}^p a\subscript{i} \lambda\subscript{i}. \nonumber
\end{eqnarray}
$$&lt;/p&gt;

&lt;p&gt;It follows that the estimator $\sum\subscript{i=1}^n \lambda\subscript{i} z\subscript{i}$ is also the &lt;em&gt;minimum risk equivariant&lt;/em&gt; (MRE) estimator of $\sum\subscript{i=1}^n \lambda\subscript{i} \eta\subscript{i}$ (with the loss function $L(\eta, d) = \rho(d - \sum \lambda\subscript{i} \eta\subscript{i})$, where $\rho$ is convex and even). Moreover, it can be shown that $s^2 / (n-p+2)$ is MRE for $\sigma^2$ under the loss function $(d-\sigma^2)^2 / \sigma^4$ (see problem 4.3 in Chapter 3 TPE).&lt;/p&gt;

&lt;p&gt;We refer to Theorem 4.3 in Chapter 3 TPE and anything referenced to from therein for more rigour and detail.&lt;/p&gt;

&lt;h3&gt;UMVU and MRE estimators in the original space&lt;/h3&gt;

&lt;p&gt;We have shown the UMVU and MRE estimators in terms of $z$, the orthogonally transformed version of $y$. However, it would be more useful to have UMVU and MRE estimators in terms of the original variables $y$.&lt;/p&gt;

&lt;p&gt;As is well-known, the &lt;em&gt;least squares estimator&lt;/em&gt; of $\mathrm{E}(y) = \xi$ is given by $\hat{y} = X (X^T X)^{-1} X^T y$, which is an orthogonal projection of $y$ on $\Pi$. It can be found by minimizing the least squares $\|y - \xi\|\subscript{2}^2 = \|y - X\beta\|\subscript{2}^2$. We have that&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\label{eq:least_squares}
\sum\subscript{i=1}^n (y\subscript{i} - \xi\subscript{i})^2 = \sum\subscript{i=1}^p (z\subscript{i} - \eta\subscript{i})^2 + \sum\subscript{i=p+1}^n z\subscript{i}^2.
\end{equation}
$$&lt;/p&gt;

&lt;p&gt;Since the left hand side is minimized by $\hat{y}$ and the right hand side is minimized by $\hat{\eta}\subscript{i} = z\subscript{i}$ for $i = 1,\dots,p$ (and $=0$ for $i&amp;gt;p$), it holds that $\hat{y} = Q^T\hat{\eta}$. Thus, the LSE $\hat{y}$ is a linear function of $z$, and therefore the estimator $\sum\subscript{i=1}^n \lambda\subscript{i} \hat{y}\subscript{i}$ is UMVU for $\sum\subscript{i=1}^n \lambda\subscript{i} \xi\subscript{i}$ by the argumentation given above (namely because $\sum\subscript{i=1}^n \lambda\subscript{i} z\subscript{i}$ is UMVU for $\sum\subscript{i=1}^n \lambda\subscript{i} \eta\subscript{i}$).
For more detail see Chapter 3 Theorem 4.4 in TPE.&lt;/p&gt;

&lt;p&gt;Likewise, it follows from the argumentation given above for the case of the orthogonal transform $z$ that the estimator $\sum\subscript{i=1}^n \lambda\subscript{i} \hat{y}\subscript{i}$ is MRE for $\sum\subscript{i=1}^n \lambda\subscript{i} \xi\subscript{i}$ under the transformation
$y^\prime = y + b$ with $b\in\Pi$ and with the loss function $L(\xi, d) = \rho(d - \sum \lambda\subscript{i} \xi\subscript{i})$ provided $\rho$ is convex and even.
See Chapter 3 Corollary 4.5 for detail.&lt;/p&gt;

&lt;p&gt;Similarly, using the results given above for the orthogonal transform $z$, by reexpressing 
$s^2 = \sum\subscript{i=p+1}^n z\subscript{i}^2 = \sum\subscript{i=1}^n (y\subscript{i} - \hat{y}\subscript{i})^2$ (from equation ($\ref{eq:least_squares}$)) it follows that the UMVU and MRE estimators of $\sigma^2$ are given by
$\sum\subscript{i=1}^n (y\subscript{i} - \hat{y}\subscript{i})^2 / (n-p)$ and $\sum\subscript{i=1}^n (y\subscript{i} - \hat{y}\subscript{i})^2 / (n-p+2)$ respectively.&lt;/p&gt;

&lt;p&gt;Finally, the LSE $\hat{\beta} = (X^T X)^{-1}X^T y$ is UMVU and MRE for $\beta$ by the above argumentation, because it can be written as a linear function of $\hat{y}$.&lt;/p&gt;

&lt;h2&gt;The least squares estimator is BLUE&lt;/h2&gt;

&lt;p&gt;A &lt;em&gt;best linear unbiased estimator&lt;/em&gt; (BLUE) is an unbiased estimator that is linear in $y$ and achieves uniformly the smallest variance among all other linear unbiased estimators (i.e., UMVU among all linear estimators).&lt;/p&gt;

&lt;p&gt;In the context of linear models, an advantage of this optimality criterion over the notions of UMVU and MRE is that it does not rely on the normality assumptions. That is, we merely need to assume that $\mathrm{E}(y) = \xi = X\beta$ and $\mathrm{Cov}(y) = \sigma^2 I$, without any further assumptions on the distribution.&lt;/p&gt;

&lt;p&gt;Assume we aim to estimate $\sum\subscript{i=1}^n \lambda\subscript{i} \xi\subscript{i} = \lambda^T X\beta$. By linearity the estimator should have the form $\delta(y) = a^T y$. Unbiasedness implies that $a^T X \beta = \lambda^T X \beta$, from which it follows that $X^T (a - \lambda) \perp \beta$ for any $\beta\in\mathbb{R}^p$, and consequently $X^T a = X^T \lambda$. Taking $m\in\mathbb{R}^n$ to be a vector of Lagrange multipliers, the minimization problem becomes,&lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
a + Xm &amp;amp;=&amp;amp; 0 \nonumber \\\
X^T a &amp;amp;=&amp;amp; X^T \lambda. \nonumber
\end{eqnarray}
$$&lt;/p&gt;

&lt;p&gt;It is easily seen that this is solved by $a = X(X^T X)^{-1} X^T \lambda$ and $m = -(X^T X)^{-1} X^T \lambda$. In particular, $\hat{\beta} = (X^T X)^{-1} X^T y$ is BLUE for $\beta$.&lt;/p&gt;

&lt;p&gt;TPE has a different approach of proving that LSE is BLUE (see Theorem 4.12 in Chapter 3, which TPE calls Gauss&amp;#39; Theorem on Least Squares). Moreover, it follows that LSE is also MRE among all linear estimators (see Corollary 4.13 in Chapter 3 TPE).&lt;/p&gt;
</description>
        <pubDate>Sun, 25 Oct 2015 00:00:00 -0500</pubDate>
        <link>http://www.alexejgossmann.com//Lehmanns_TSH_and_TPE/LSE_so_nice/</link>
        <guid isPermaLink="true">http://www.alexejgossmann.com//Lehmanns_TSH_and_TPE/LSE_so_nice/</guid>
        
        
      </item>
    
      <item>
        <title>Least squares estimators are nice! PART 2 (consistency, asymptotic normality and efficiency)</title>
        <description>&lt;p&gt;Chapter 6 in TPE introduces measures of asymptotic optimality of estimators, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Consistent_estimator&quot;&gt;consistency&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Efficient_estimator&quot;&gt;efficiency&lt;/a&gt; as the sample size gets large. Here, I aim to apply these results in the setting of linear models.&lt;/p&gt;

&lt;h2&gt;Overview of the results on asymptotics in TPE&lt;/h2&gt;

&lt;p&gt;Theorem 6.3.7 establishes the existence of a consistent sequence of roots of the log-likelihood in a one-parameter family of distributions (i.e. a sequence of estimators $\hat{\theta}\subscript{n}$ such that $\hat{\theta}\subscript{n} \rightarrow \theta$ in probability, where $\theta$ denotes the true parameter to be estimated). The Theorem does not tell us how exactly to pick a consistent sequence, unless the log-likelihood has a unique root for all $n$, in which case the sequence of maximum likelihood estimators is consistent (see Corollary 6.3.8). &lt;/p&gt;

&lt;p&gt;Theorem 6.3.10 extends the previous result by establishing that any consistent sequence of roots of the log-likelihood is asymptotically normal with asymptotic variance given by $\frac{1}{\mathcal{I}(\theta)}$, the inverse of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Fisher_information&quot;&gt;Fisher&amp;#39;s information&lt;/a&gt;. Such a sequence $\hat{\theta}\subscript{n}$ is called an &lt;em&gt;efficient likelihood estimator&lt;/em&gt;. In particular, $\hat{\theta}\subscript{n}$ is also &lt;em&gt;asymptotically efficient&lt;/em&gt;, meaning that it attains the &lt;a href=&quot;https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound&quot;&gt;Cramér–Rao lower bound&lt;/a&gt; asymptotically.&lt;/p&gt;

&lt;p&gt;Finally, Theorem 6.5.1 extends the previous results to the multi-parameter case. When the true parameter $\theta$ is a vector, there exist solutions $\hat{\theta}\subscript{n}$ to the likelihood equation (under some additional regularity conditions), such that $\hat{\theta}\subscript{jn} \rightarrow \theta\subscript{j}$ in probability and $\sqrt{n}(\hat{\theta}\subscript{n} - \theta) \overset{\mathcal{L}}{\rightarrow} \mathcal{N}(0,\mathcal{I}(\theta)^{-1})$. In particular, $\hat{\theta}\subscript{jn}$ is asymptotically efficient for each $j$.&lt;/p&gt;

&lt;p&gt;Additionally, TPE presents multiple results on how to find a consistent estimator when the log-likelihood has multiple roots. For the one-parameter case see Theorem 6.4.3, Corollary 6.4.4 and Examples 6.4.6, 6.4.7 and 6.4.10, as well as other results in Section 4 of Chapter 6 in TPE. These results are extended to the multi-parameter situation in Theorem 6.5.3 and Corollary 6.5.4.&lt;/p&gt;

&lt;h2&gt;Application to a stochastic linear model&lt;/h2&gt;

&lt;p&gt;The usual formulation of a linear model is $y\subscript{i} = x\subscript{i}^T \beta + \varepsilon\subscript{i}$, where $\varepsilon\subscript{i}$ are i.i.d. error terms, and the $x\subscript{i}$ are (deterministic) measurements of the predictor variables for the $i$th subject. The above results on asymptotics cannot be applied to this model, because the observations $y\subscript{i}$ are not identically distributed. However, if we consider $x\subscript{i}$ to be stochastic and i.i.d., then the tupels $(y\subscript{i}, x\subscript{i})$ are i.i.d. observations of a random vector $(y, x)$, and consequently the above theory is applicable.&lt;/p&gt;

&lt;p&gt;Thus, we consider the following model formulation:&lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
y\subscript{i} | x\subscript{i} &amp;amp;=&amp;amp; x\subscript{i}^T \beta + \varepsilon\subscript{i}, \quad y\subscript{i}, \varepsilon\subscript{i} \in \mathbb{R}, x\subscript{i}, \beta \in \mathbb{R}^p, \nonumber \\\
x\subscript{i} &amp;amp;\sim&amp;amp; \mathrm{i.i.d.}\,F\subscript{\xi}, \quad \varepsilon\subscript{i} \sim \mathrm{i.i.d.}\,\mathcal{N}(0,\sigma^2), \quad x\subscript{i} \,\mathrm{and}\, \varepsilon\subscript{i} \,\mathrm{are\,independent}, \nonumber \\\
\mathrm{E}(x\subscript{i}) &amp;amp;=&amp;amp; \int z \mathrm{d}F\subscript{\xi}(z) \,\mathrm{and}\, \mathrm{E}(x\subscript{i}x\subscript{i}^T) = \int zz^T \mathrm{d}F\subscript{\xi}(z) \,\mathrm{both\,exist}. \nonumber
\end{eqnarray}
$$&lt;/p&gt;

&lt;p&gt;Writing $y = (y\subscript{1}, \dots, y\subscript{N})^T \in \mathbb{R}^{N}$, $X = (x\subscript{1}, \dots, x\subscript{N})^T \in \mathbb{R}^{N\times p}$ and $f(\cdot; \xi) = \frac{\partial}{\partial x} F\subscript{\xi}(x)$, it follows that the log-likelihood is given by&lt;/p&gt;

&lt;p&gt;$$l(\beta, \sigma^2; \xi) = -\frac{N}{2}\log(2\pi) - \frac{N}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\|y - X\beta\|\subscript{2}^2 + \sum \log f(x\subscript{i}; \xi).$$&lt;/p&gt;

&lt;p&gt;It is apparent that $\xi$ is asymptotically uncorrelated with $(\beta, \sigma^2)$. Thus, the estimation of $(\beta, \sigma^2)$ and $\xi$ can be performed separately without efficiency loss. &lt;/p&gt;

&lt;h3&gt;Maximum likelihood estimators and information matrix&lt;/h3&gt;

&lt;p&gt;Using basic rules of differentiation of matrices, vectors and scalar forms we arrive at&lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
\frac{\partial l}{\partial \beta} &amp;amp;=&amp;amp; -\frac{1}{\sigma^2} (X^T X \beta - X^T y), \nonumber \\\
\frac{\partial l}{\partial \sigma^2} &amp;amp;=&amp;amp; -\frac{N}{2\sigma^2} + \frac{\|y - X\beta\|\subscript{2}^2}{2(\sigma^2)^2}, \nonumber \\\
\frac{\partial^2 l}{\partial \beta \partial \beta} &amp;amp;=&amp;amp; -\frac{1}{\sigma^2} X^T X, \nonumber \\\
\frac{\partial^2 l}{\partial \beta \partial \sigma^2} &amp;amp;=&amp;amp; \frac{\partial^2 l}{\partial \sigma^2 \partial \beta} = \frac{1}{(\sigma^2)^2} (X^T X \beta - X^T y), \nonumber \\\
\frac{\partial^2 l}{\partial \sigma^2 \partial \sigma^2} &amp;amp;=&amp;amp; \frac{N}{2(\sigma^2)^2} - \frac{\|y - X\beta\|\subscript{2}^2}{(\sigma^2)^3}. \nonumber
\end{eqnarray}
$$&lt;/p&gt;

&lt;p&gt;Setting the first derivatives to zero, it follows that the maximum likelihood estimators are&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\label{LSE}
\hat{\beta} = (X^T X)^{-1} X^T y,
\end{equation}
$$&lt;/p&gt;

&lt;p&gt;which is also known as the least squares estimator of $\beta$, and&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\label{MSE}
\hat{\sigma}^2 = \frac{\|y - X\beta\|\subscript{2}^2}{N}.
\end{equation}
$$&lt;/p&gt;

&lt;p&gt;In order to obtain the information matrix we take expectations of the second derivatives:&lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
\mathrm{E}\left(\frac{\partial^2 l}{\partial \beta \partial \beta}\right) &amp;amp;=&amp;amp; -\frac{1}{\sigma^2} \sum\subscript{i=1}^N \mathrm{E}(x\subscript{i} x\subscript{i}^T) = - \frac{N}{\sigma^2} \int zz^T \mathrm{d}F\subscript{\xi}(z), \nonumber \\\
\mathrm{E}\left(\frac{\partial^2 l}{\partial \beta \partial \sigma^2}\right) &amp;amp;=&amp;amp; \frac{1}{(\sigma^2)^2} \left[ \mathrm{E}(X^T X \beta) - \mathrm{E}(\mathrm{E}(X^T y |X))\right] \nonumber \\\
&amp;amp;=&amp;amp; \frac{1}{(\sigma^2)^2} \left[ \mathrm{E}(X^T X \beta) - \mathrm{E}(X^T X \beta) \right] = 0, \nonumber \\\
\mathrm{E}\left(\frac{\partial^2 l}{\partial \sigma^2 \partial \sigma^2}\right) &amp;amp;=&amp;amp; \frac{N}{2(\sigma^2)^2} - \frac{\mathrm{E}(\mathrm{E}(\|y - X\beta\|\subscript{2}^2 | X))}{(\sigma^2)^3} \nonumber \\\
&amp;amp;=&amp;amp; \frac{N}{2(\sigma^2)^2} - \frac{\mathrm{E}(\|X \beta + \varepsilon - X\beta\|\subscript{2}^2)}{(\sigma^2)^3} \nonumber \\\
&amp;amp;=&amp;amp; \frac{N}{2(\sigma^2)^2} - \frac{N\sigma^2}{(\sigma^2)^3} = - \frac{N}{2(\sigma^2)^2}. \nonumber
\end{eqnarray}
$$&lt;/p&gt;

&lt;p&gt;Consequently, the information of $(\beta, \sigma^2)$ is given by&lt;/p&gt;

&lt;p&gt;$$
\mathcal{I}\left( \begin{pmatrix} \beta \\ \sigma^2 \end{pmatrix} \right) = 
\begin{pmatrix}
\frac{N}{\sigma^2} \int zz^T \mathrm{d}F\subscript{\xi}(z) &amp;amp; 0 \\
0 &amp;amp; \frac{N}{2(\sigma^2)^2}
\end{pmatrix}.
$$&lt;/p&gt;

&lt;h3&gt;Consistency, asymptotic normality and efficiency&lt;/h3&gt;

&lt;p&gt;In the model considered above, $(\beta, \sigma^2)$ parametrize distinct distributions with a common support, and the observations $(y\subscript{i}, x\subscript{i}, \varepsilon\subscript{i})$ are i.i.d. Therefore, we can use the standard asymptotic theory presented in Chapter 6 of TPE.&lt;/p&gt;

&lt;h4&gt;Assumptions&lt;/h4&gt;

&lt;p&gt;Still, in order to apply Theorem 5.1 of Section 6 in TPE, a number of additional conditions have to be verified. These are given as (A), (B), (C) and (D) in Section 5 of Chapter 6, and are checked in the following.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(A)&lt;/strong&gt; This is the assumption that the true parameter $(\beta, \sigma^2)$ is contained in a small open neighborhood, on which $l$ is three times differentiable. Unless the above model is degenerate (i.e. $\sigma^2 = 0$), the assumption should be valid.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(B)&lt;/strong&gt; Here we need to verify:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;$\mathrm{E}\left(\frac{\partial l}{\partial \beta}\right) = -\frac{1}{\sigma^2} \mathrm{E}(X^T X \beta - X^T y) = 0$.&lt;/li&gt;
&lt;li&gt;$\mathrm{E}\left(\frac{\partial l}{\partial \sigma^2}\right) = -\frac{N}{2\sigma^2} + \frac{\mathrm{E}\|y - X\beta\|\subscript{2}^2}{2(\sigma^2)^2} = 0$.&lt;/li&gt;
&lt;li&gt;&lt;p&gt;$\forall j,k \in \{1,\dots,p+1\} : \mathrm{E}\left(-\frac{\partial^2 l}{\partial \theta\subscript{j} \partial \theta\subscript{k}} \right)
= \mathrm{E}\left(\frac{\partial l}{\partial \theta\subscript{j}} \cdot \frac{\partial l}{\partial \theta\subscript{k}} \right)$, where $\theta := (\beta, \sigma^T)^T$.
  This easily follows by observing that &lt;/p&gt;

&lt;p&gt;$$\frac{\partial^2 l}{\partial \theta\subscript{j} \partial \theta\subscript{k}} = \frac{\partial^2}{\partial \theta\subscript{j} \partial \theta\subscript{k}} \log(g) = \frac{\frac{\partial^2 g}{\partial \theta\subscript{j} \partial \theta\subscript{k}}}{g} - \frac{\frac{\partial g}{\partial \theta\subscript{j}} \frac{\partial g}{\partial \theta\subscript{k}}}{g^2} = \frac{\frac{\partial^2 g}{\partial \theta\subscript{j} \partial \theta\subscript{k}}}{g} - \frac{\partial l}{\partial \theta\subscript{j}} \frac{\partial l}{\partial \theta\subscript{k}},$$&lt;/p&gt;

&lt;p&gt;where $g$ is the joint density of $(y\subscript{i}, x\subscript{i})$, and then taking the expectation of both sides.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;(C)&lt;/strong&gt; The information matrix $\mathcal{I}\left( \begin{pmatrix} \beta \\ \sigma^2 \end{pmatrix} \right)$ has to be positive definite. It is a covariance matrix and therefore at least positive semi-definite. In practice, positive definiteness can be safely assumed, because a covariance matrix $\mathrm{Cov}(z)$ is only positive semi-definite rather than positive definite if and only if $a^T z$ constant with probability 1 for some vector $a$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(D)&lt;/strong&gt; This condition essentially requires that the third derivatives of the log-likelihood should be bounded in a small neighborhood of the true parameter $(\beta, \sigma^2)$. By looking at the previously computed second derivatives, it is clear that the condition is satisfied.&lt;/p&gt;

&lt;h4&gt;Conclusion&lt;/h4&gt;

&lt;p&gt;Thus, all assumptions of Theorem 6.5.1 in TPE are satisfied, and it follows the conclusion:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;The maximum likelihood estimators ($\ref{LSE}$) and ($\ref{MSE}$) are consistent for estimating $\beta$ and $\sigma^2$. That is, the well-known least squares estimator $\hat{\beta}$ converges in probability to the true parameter $\beta$, and $\hat{\sigma}^2$ converges in probability to the true $\sigma^2$.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The derived maximum likelihood estimators ($\ref{LSE}$) and ($\ref{MSE}$) are asymptotically normal:&lt;/p&gt;

&lt;p&gt;$$\mathcal{I}\left( \begin{pmatrix} \beta \\ \sigma^2 \end{pmatrix} \right)^{1/2} \left[ \begin{pmatrix} \hat{\beta} \\ \hat{\sigma}^2 \end{pmatrix} - \begin{pmatrix} \beta \\ \sigma^2 \end{pmatrix} \right]
\overset{\mathcal{L}}{\longrightarrow} \mathcal{N}(0, I),$$ &lt;/p&gt;

&lt;p&gt;where&lt;/p&gt;

&lt;p&gt;$$
\mathcal{I}\left( \begin{pmatrix} \beta \\ \sigma^2 \end{pmatrix} \right) = 
\begin{pmatrix}
\frac{N}{\sigma^2} \int zz^T \mathrm{d}F\subscript{\xi}(z) &amp;amp; 0 \\
0 &amp;amp; \frac{N}{2(\sigma^2)^2}
\end{pmatrix}.
$$&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;The estimator $\hat{\sigma}^2$ as well as all estimators $\hat{\beta}\subscript{i}$ (for each $i\in\{1,\dots,p\}$) are asymptotically efficient in the sense that each of those achieves its respective &lt;a href=&quot;https://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound&quot;&gt;Cramér–Rao lower bound&lt;/a&gt; asymptotically.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4&gt;Alternative proof&lt;/h4&gt;

&lt;p&gt;In Section 3.6.2 of Demidenko (2013) &amp;quot;Mixed Models: Theory and Applications with R&amp;quot; (2nd ed.), consistency and asymptotic normality of the least squares estimator in a linear model with stochastic predictors $x\subscript{i}$ is established based on the Law of Large Numbers and a multivariate version of the Central Limit Theorem.&lt;/p&gt;

&lt;h2&gt;Asymptotics of a deterministic linear model&lt;/h2&gt;

&lt;p&gt;Even though, for reasons delineated above, the theory of Chapter 6 in TPE cannot be applied when the predictors $x\subscript{i}$ are considered deterministic rather than random, asymptotic properties of the estimators can still be established.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;https://projecteuclid.org/euclid.aos/1176346597&quot;&gt;Fahrmeir and Kaufmann (1985) &amp;quot;Consistency and Asymptotic Normality of the Maximum Likelihood Estimator in Generalized Linear Models&amp;quot;&lt;/a&gt; under some regularity conditions give a proof of the weak and strong consistency as well as asymptotic normality of estimators in generalized linear models, of which the linear model is a simple special case.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC392707/pdf/pnas00019-0030.pdf&quot;&gt;Robbins and Wei (1978) &amp;quot;Strong consistency of least squares estimates in multiple regression&amp;quot;&lt;/a&gt; prove that the least squares estimator $\hat{\beta} = (X^T X)^{-1} X^T y$ is consistent if $(X^T X)^{-1} \rightarrow 0$ as $N\rightarrow \infty$, without any distributional assumptions on $y$ apart from independence. &lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Section 13.1.1 in Demidenko (2013) &amp;quot;Mixed Models: Theory and Applications with R&amp;quot; (2nd ed.) presents the following proof of asymptotic normality of the least squares estimator.&lt;/p&gt;

&lt;p&gt;Consider the model $y\subscript{i} = x\subscript{i}^T \beta + \varepsilon\subscript{i}$ for $i = 1,\ldots,N$, where $y\subscript{i}, \varepsilon\subscript{i} \in \mathbb{R}$ and $x\subscript{i}, \beta \in \mathbb{R}^p$.
Assume that the $\varepsilon\subscript{i}$ are i.i.d. with $\mathrm{E}(\varepsilon\subscript{i}) = 0$ and $\mathrm{Var}(\varepsilon\subscript{i}) = \sigma^2$.
Assume that $\lim N^{-1} \sum\subscript{i=1}^N x\subscript{i}x\subscript{i}^T = V$, and that there is a constant $c$ such that $\|x\subscript{i}\| \leq c$.
Then by a multivariate version of the Central Limit Theorem it holds that&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\label{CLT}
\frac{1}{\sqrt{N}} \sum\subscript{i=1}^N x\subscript{i}\varepsilon\subscript{i} \overset{\mathcal{L}}{\longrightarrow} \mathcal{N}(0, \sigma^2 V).
\end{equation}
$$&lt;/p&gt;

&lt;p&gt;The least squares estmator can be expressed as &lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\label{OLS}
\hat{\beta} = \left(\sum\subscript{i=1}^N x\subscript{i} x\subscript{i}^T \right)^{-1} \left(\sum\subscript{i=1}^N x\subscript{i} y\subscript{i} \right).
\end{equation}
$$ &lt;/p&gt;

&lt;p&gt;Together, $(\ref{CLT})$ and $(\ref{OLS})$ imply that&lt;/p&gt;

&lt;p&gt;$$\hat{\beta} \overset{\mathcal{L}}{\longrightarrow} \mathcal{N}\left(\beta, \sigma^2 \left(\sum\subscript{i=1}^N x\subscript{i} x\subscript{i}^T \right)^{-1} \right).$$&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is also worth pointing out that under a normality assumption on the error terms, (small sample) distributions of the estimators $\hat{\beta}$ and $\hat{\sigma}^2$ can be easily derived (in particular, $\hat{\beta} \sim \mathcal{N}(\beta, \sigma^2 (X^T X)^{-1})$ and $\frac{\|y - X\hat{\beta}\|\subscript{2}^2}{\sigma^2} \sim \chi\subscript{N-p}^2$). The normality assumption in fact seems currently to be the standard in the statistical modeling literature. Moreover, under the normality assumption the estimators fulfill further optimality assumptions, such as &lt;a href=&quot;/Lehmanns_TSH_and_TPE/LSE_so_nice/&quot;&gt;uniform minimum variance unbiasedness and minimum risk equivariance&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Sat, 24 Oct 2015 00:00:00 -0500</pubDate>
        <link>http://www.alexejgossmann.com//Lehmanns_TSH_and_TPE/LSE_so_nice_2/</link>
        <guid isPermaLink="true">http://www.alexejgossmann.com//Lehmanns_TSH_and_TPE/LSE_so_nice_2/</guid>
        
        
      </item>
    
      <item>
        <title>Small sample tests of linear hypotheses in linear models</title>
        <description>&lt;p&gt;Consider a linear model $y = X\beta + \varepsilon$, where $y\in\mathbb{R}^n$, $X\in\mathbb{R}^{n\times p}$ with $p &amp;lt; n$, $\mathrm{rank}(X) = p$, $\beta\in\mathbb{R}^p$, and $\varepsilon\subscript{i} \sim \mathrm{i.i.d.}\, \mathcal{N}(0,\sigma^2)$ for all $i\in\{1,\dots,n\}$.&lt;/p&gt;

&lt;p&gt;For convenience, denote $\xi := \mathrm{E}(y) = X\beta$. It holds that $\xi\in\Pi$, where $\Pi$ denotes a $p$-dimensional subspace of $\mathbb{R}^n$ (spanned by the columns of $X$).&lt;/p&gt;

&lt;p&gt;Assume that we want to test a hypothesis of the form $\mathrm{H} : C^T \beta = 0$, where $C \in \mathbb{R}^{p\times r}$ with $r &amp;lt; p$ and $\mathrm{rank}(C) = r$. It holds that $C^T = B^T X$ where $B^T = C^T(X^T X)^{-1} X^T \in \mathbb{R}^{r\times n}$. Thus, we can rewrite the hypothesis as $\mathrm{H} : B^T \xi = 0$. That is, under the null hypothesis $\xi$ lies in a $(p-r)$-dimensional subspace of $\Pi$. If we denote this $(p-r)$-dimensional subspace by $\omega$, then the testing problem becomes&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\label{test}
\mathrm{H} : \xi \in \omega \quad\mathrm{vs.}\quad \mathrm{K} : \xi\in\Pi.
\end{equation}
$$&lt;/p&gt;

&lt;p&gt;In the following we will construct the uniformly most powerful test among all invariant tests for testing problems of this general form.&lt;/p&gt;

&lt;h2&gt;Orthogonal coordinate transformation&lt;/h2&gt;

&lt;p&gt;Let $Q\in\mathbb{R}^{n\times n}$ be an orthogonal matrix such that its first $p$ rows $q\subscript{1}, \dots, q\subscript{p}$ span $\Pi$, and such that $\mathrm{span}(q\subscript{r+1}, \dots, q\subscript{p}) = \omega$.&lt;/p&gt;

&lt;p&gt;Let $z := Qy$, and denote $\eta := \mathrm{E}(z) = Q\xi$. It follows that &lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$z\subscript{p+1} = \dots = z\subscript{n} = 0$ if and only if $y\in\Pi$,&lt;/li&gt;
&lt;li&gt;$z\subscript{1} = \dots = z\subscript{r} = 0$ and $z\subscript{p+1} = \dots = z\subscript{n} = 0$ if and only if $y\in\omega$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then the hypothesis H can be given can be given in a simpler form:&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\label{orthotest}
\mathrm{H} : \eta\subscript{1} = \dots = \eta\subscript{r} = 0.
\end{equation}
$$&lt;/p&gt;

&lt;h2&gt;Reduction of the problem via the principle of invariance&lt;/h2&gt;

&lt;p&gt;In general, we call a testing problem $\mathrm{H} : \xi \in \Omega\subscript{H}$ vs. $\mathrm{K} : \xi\in\Omega\subscript{K}$ &lt;em&gt;invariant&lt;/em&gt; under a transformation $g$ of the sample space, if the induced transformation $\bar{g}$ on the parameter space preserves both $\Omega\subscript{H}$ and $\Omega\subscript{K}$, that is, if $\bar{g}\Omega = \Omega$, $\bar{g}\Omega\subscript{H} = \Omega\subscript{H}$ and $\bar{g}\Omega\subscript{K} = \Omega\subscript{K}$. See Section 6.1 in TSH for a more detailed definition. In the following, we denote by $G$ the group of all such transformations.&lt;/p&gt;

&lt;p&gt;In the present case, the testing problem ($\ref{orthotest}$) remains invariant under the groups of transformations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;All transformations of the form $z\subscript{i}^\prime = z\subscript{i} + c\subscript{i}$ with any $c\subscript{i}\in\mathbb{R}^{p-r}$ for $i = \{r+1, \ldots, p\}$.&lt;/li&gt;
&lt;li&gt;All orthogonal transformations of $z\subscript{1}, \ldots, z\subscript{r}$.&lt;/li&gt;
&lt;li&gt;All scale changes $z^\prime = cz$ with any $c\in\mathbb{R}$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A function which is constant on each &lt;a href=&quot;https://en.wikipedia.org/wiki/Group_action#Orbits_and_stabilizers&quot;&gt;orbit&lt;/a&gt; of $G$, but takes on a different value for each orbit is called &lt;em&gt;maximal invariant&lt;/em&gt;. See Section 6.2 in TSH for a more detailed definition.&lt;/p&gt;

&lt;p&gt;In the present case, it is easy to derive (see Section 7.1 in TSH for a step-by-step derivation) that a maximal invariant under the above transformations is given by&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\label{orthoteststat}
W = \frac{\sum\subscript{i=1}^r z\subscript{i}^2 / r}{\sum\subscript{i=p+1}^n z\subscript{i}^2 / (n-p)},
\end{equation}
$$ &lt;/p&gt;

&lt;p&gt;and a maximal invariant in the parameter space is given by&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\label{psi}
\psi^2 = \frac{\sum\subscript{i=1}^r \eta\subscript{i}^2}{\sigma^2}.
\end{equation}
$$ &lt;/p&gt;

&lt;p&gt;Thus, the principle of invariance reduces the problem ($\ref{test}$) to&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\label{simplifiedtest}
\mathrm{H} : \psi^2 = 0 \quad\mathrm{vs.}\quad \mathrm{K} : \psi^2 &amp;gt; 0.
\end{equation}
$$&lt;/p&gt;

&lt;h2&gt;Uniformly most powerful invariant test&lt;/h2&gt;

&lt;p&gt;Since $z \sim \mathcal{N}(\eta, \sigma^2 I)$, the expression in ($\ref{orthoteststat}$) makes it clear that $W \sim F\subscript{n-p}^r$ under the null hypothesis.&lt;/p&gt;

&lt;p&gt;Now, Theorem 6.3.2 in TSH implies that the distribution of $W$ only depends on $\psi^2$.
Additionally, it can be shown (see Problems 7.2 and 7.3 in TSH) that the likelihood ratio $\frac{p\subscript{\psi\subscript{1}}(w)}{p\subscript{0}(w)}$ is increasing in $w$ for any $\psi\subscript{1}$. Therefore, by the variant of the Neyman-Pearson fundamental lemma given in Theorem 3.4.1 in TSH, it follows that the uniformly most powerful invariant test of ($\ref{simplifiedtest}$) rejects H if and only if $W &amp;gt; C$, where $C$ is determined by&lt;/p&gt;

&lt;p&gt;$$\int\subscript{C}^\infty F\subscript{n-p}^r(w) \mathrm{d}w = \alpha.$$&lt;/p&gt;

&lt;p&gt;Moreover, it is worth pointing out that in the case that $r=1$, the test reduces to a two-sided $t$-test&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\label{orthottest}
t = \frac{|z\subscript{1}|}{\sqrt{\sum\subscript{i=p+1}^n z\subscript{i}^2 / (n-p)}} &amp;gt; C\subscript{0}.
\end{equation}
$$&lt;/p&gt;

&lt;p&gt;This $t$-test is not only UMP invariant but also UMP unbiased (see Problem 5.5 in TSH).&lt;/p&gt;

&lt;h3&gt;Test statistic in terms of the original variables&lt;/h3&gt;

&lt;p&gt;Of course, in practice it is rather inconvenient having to find the orthogonal transform $z = Qy$. Therefore, we re-express the test statistic in terms of the original variables $y$.&lt;/p&gt;

&lt;p&gt;Let $P = X(X^T X)^{-1}X^T$ and $\hat{\xi} = Py$. That is, $P$ is the orthogonal projection onto $\Pi$, and $\hat{\xi}$ is the least squares estimator of $\mathrm{E}(y) = \xi$.&lt;/p&gt;

&lt;p&gt;Then $y - \hat{\xi}$ is orthogonal to $\Pi$ and $\hat{\xi}$ is orthogonal to $\Pi^c$. Therefore and by the orthogonality of $Q$ we have that&lt;/p&gt;

&lt;p&gt;$$\sum\subscript{i=p+1}^n z\subscript{i}^2 = \|Q(y - \hat{\xi})\|\subscript{2}^2 = \|y - \hat{\xi}\|\subscript{2}^2.$$&lt;/p&gt;

&lt;p&gt;Similarly we conclude that&lt;/p&gt;

&lt;p&gt;$$\sum\subscript{i=1}^r z\subscript{i}^2 + \sum\subscript{i=p+1}^n z\subscript{i}^2 = \|y - \hat{\hat{\xi}}\|\subscript{2}^2,$$&lt;/p&gt;

&lt;p&gt;where $\hat{\hat{\xi}}$ is the projection of $y$ onto $\omega$.&lt;/p&gt;

&lt;p&gt;Thus, we can write the test statistic ($\ref{orthoteststat}$) as&lt;/p&gt;

&lt;p&gt;$$
\begin{equation}
\label{teststat}
W = \frac{\left[\|y - \hat{\hat{\xi}}\|\subscript{2}^2 - \|y - \hat{\xi}\|\subscript{2}^2 \right] / r}{\|y - \hat{\xi}\|\subscript{2}^2 / (n-p)} = \frac{\|\hat{\xi} - \hat{\hat{\xi}}\|\subscript{2}^2 / r}{\|y - \hat{\xi}\|\subscript{2}^2 / (n-p)}.
\end{equation}
$$ &lt;/p&gt;

&lt;h3&gt;Example: Derivation of the well-known t-test&lt;/h3&gt;

&lt;p&gt;Assume for some $c\in\mathbb{R}^p$ (i.e. $r=1$) we want to test $\mathrm{H} : c^T \beta = 0$. As discussed in the very beginning this test is equivalent to testing $\mathrm{H} : b^T \xi = 0$ where $b^T = c^T(X^T X)^{-1}X^T$. It follows that &lt;/p&gt;

&lt;p&gt;$$
\begin{eqnarray}
\hat{\xi} &amp;amp;=&amp;amp; X(X^T X)^{-1} X^T y, \nonumber \\\
\hat{\hat{\xi}} &amp;amp;=&amp;amp; \hat{\xi} - \frac{b^T y}{\|b\|\subscript{2}^2} b, \nonumber \\\
&amp;amp;=&amp;amp; \hat{\xi} - \frac{c^T \hat{\beta}}{\|b\|\subscript{2}^2} b, \nonumber 
\end{eqnarray}
$$&lt;/p&gt;

&lt;p&gt;where $\hat{\beta} = (X^T X)^{-1} X^T y$ is the usual least squares solution. Then it holds that&lt;/p&gt;

&lt;p&gt;$$
\|\hat{\xi} - \hat{\hat{\xi}}\|\subscript{2}^2 = \left\|\frac{c^T \hat{\beta}}{\|b\|\subscript{2}^2} b \right\|\subscript{2}^2 = \frac{(c^T \hat{\beta})^2}{\|b\|\subscript{2}^2},
$$&lt;/p&gt;

&lt;p&gt;and consequently ($\ref{teststat}$) becomes&lt;/p&gt;

&lt;p&gt;$$W = \frac{(c^T \hat{\beta})^2}{s^2 c^T (X^T X)^{-1} c},$$&lt;/p&gt;

&lt;p&gt;where $s^2 = \|y - \hat{\xi}\|\subscript{2}^2 / (n-p)$. As shown above, $W$ has the $F$ distribution with 1 and $(n-p)$ degrees of freedom. Thus, the statistic&lt;/p&gt;

&lt;p&gt;$$t = \frac{c^T \hat{\beta}}{s \sqrt{c^T (X^T X)^{-1} c}}$$&lt;/p&gt;

&lt;p&gt;has the $t$ distribution with $(n-p)$ degrees of freedom.&lt;/p&gt;

&lt;p&gt;In particular, the hypothesis $\mathrm{H} : \beta\subscript{i} = 0$ is rejected if $|t| &amp;gt; t\subscript{\alpha/2, n-p}$, where the test statistic is given by &lt;/p&gt;

&lt;p&gt;$$t = \frac{\hat{\beta}\subscript{i}}{s \sqrt{\left[(X^T X)^{-1}\right]\subscript{i,i}}}.$$&lt;/p&gt;
</description>
        <pubDate>Fri, 23 Oct 2015 00:00:00 -0500</pubDate>
        <link>http://www.alexejgossmann.com//Lehmanns_TSH_and_TPE/small_sample_tests_for_LM/</link>
        <guid isPermaLink="true">http://www.alexejgossmann.com//Lehmanns_TSH_and_TPE/small_sample_tests_for_LM/</guid>
        
        
      </item>
    
  </channel>
</rss>
