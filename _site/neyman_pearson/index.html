<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width initial-scale=1" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <title>An informal summary of Neyman-Pearson and generalizations</title>
    <meta name="description" content="A collection of random facts I observed while reading Lehmann's  "Testing Statistical Hypotheses" and "Theory of Point Estimation"
">

    <link rel="stylesheet" href="/Lehmanns_TSH_and_TPE/css/main.css">
    <link rel="canonical" href="http://www.alexejgossmann.com//Lehmanns_TSH_and_TPE/neyman_pearson/">
 
    <!-- MathJax interation-->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [["$","$"]]}});
      MathJax.Hub.Config({TeX: {Macros:{subscript:['_{#1}',1],superscript:['^{#1}',1]}}});
    </script> 
    <!-- Turn on equation numbering -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      TeX: { equationNumbers: { autoNumber: "AMS" } }
    });
    </script>
    <script type="text/javascript"
      src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML-full">
    </script>
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/Lehmanns_TSH_and_TPE/">Some impressions from the books  "Testing Statistical Hypotheses" and "Theory of Point Estimation"</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">An informal summary of Neyman-Pearson and generalizations</h1>
  </header>

  <article class="post-content">
    <p>The following offers an informal view on the fundamental lemma of Neyman and Pearson and generalizations thereof.
For a mathematically rigorous presentation see the corresponding results in TSH, which are cited in this article.</p>

<p><em>Notation</em>: MP = &quot;most powerful&quot;, UMP = &quot;uniformly most powerful&quot;, $H$ denotes the null hypothesis, $K$ denotes the alternative hypothesis, $\alpha$ denotes the level of the hypothesis test, lower case Roman letters denote realizations of random variables (upper case).</p>

<ol>
<li><p><em>[Simple hypotheses]</em> What is actually called fundamental lemma of Neyman and Pearson in TSH (Theorem 3.2.1) is concerned with a test of two simple hypotheses. Under the null hypothesis the random variable $X$ is assumed to follow a probability distribution with density $p\subscript{0}$, while under the alternative hypothesis the density is $p\subscript{1}$. </p>

<p>Consider $H : p\subscript{0}$ vs. $K : p\subscript{1}$. MP test $\phi$ exists. It rejects the null if $\frac{p\subscript{1}}{p\subscript{0}} &gt; k$, accepts the null if $\frac{p\subscript{1}}{p\subscript{0}} &lt; k$, and rejects with probability $\gamma$ if $\frac{p\subscript{1}}{p\subscript{0}} = k$, where $\gamma$ and $k$ are chosen to satisfy $\mathrm{E}\subscript{p\subscript{0}} \phi(X) = \alpha$.</p></li>
<li><p><em>[Monotone likelihood ratio, one-sided, one-param. exp. fam.]</em> For one-parameter families of distributions and one-sided hypotheses, the Neyman-Pearson lemma can be generalized to construct a UMP test if the distributions in question have monotone likelihood ratios. This is Theorem 3.4.1 in TSH.</p>

<p>Consider $H : \theta \leq \theta\subscript{0}$ vs. $K : \theta &gt; \theta\subscript{0}$ ($\theta \in \mathbb{R}$). If $\frac{p\subscript{\theta^\prime}(x)}{p\subscript{\theta}(x)}$ is nondecreasing in $T(x)$ for any $\theta &lt; \theta^\prime$, then a UMP test $\phi$ exists. It rejects if $T(x) &gt; C$, accepts if $T(x) &lt; C$, and rejects with probability $\gamma$ if $T(x) = C$, where $C$ and $\gamma$ are determined by $\mathrm{E}\subscript{\theta\subscript{0}} \phi(X) = \alpha$.</p>

<p>By interchanging the inequalities one obtains a UMP test for the dual problem $H : \theta \geq \theta\subscript{0}$ vs. $K : \theta &lt; \theta\subscript{0}$.</p>

<p>Additionally, this test minimizes the Type I error subject to $\mathrm{E}\subscript{\theta\subscript{0}} \phi(X) = \alpha$.</p></li>
<li><p><em>[Two-sided null in one-param. exp. fam.]</em> An analogous UMP test exists for a two-sided null hypothesis $H : \theta \leq \theta\subscript{1} \,\mathrm{or}\, \theta \geq \theta\subscript{2}$ in one-parameter exponential families. It rejects if $C\subscript{1} &lt; T(x) &lt; C\subscript{2}$, accepts if $T(x) &lt; C\subscript{1}$ or $T(x) &gt; C\subscript{2}$, rejects with probability $\gamma$ if $T(x) = C\subscript{i}$ (for $i=1$ or $i=2$), and satisfies $\mathrm{E}\subscript{\theta\subscript{1}} \phi(X) = \alpha = \mathrm{E}\subscript{\theta\subscript{2}} \phi(X)$. Subject to the last condition, this test minimizes the Type I error. See Theorem 3.7.1 in TSH.</p>

<p>A UMP test for a two-sided alternative hypothesis $K : \theta \leq \theta\subscript{1} \,\mathrm{or}\, \theta \geq \theta\subscript{2}$ does not exist (<a href="/Lehmanns_TSH_and_TPE/two-sided_hypotheses/">e.g. see my corresponding write-up</a>). However, a UMP <em>unbiased</em> test analogous to the above exists (see Section 4.2 in TSH).</p></li>
<li><p><em>[UMP unbiased tests, multi-param. exp. fam.]</em> For multi-parameter exponential families the existence of a UMP test typically cannot be established. However, UMP <em>unbiased</em> tests can be constructed without great difficulties. Assume that $\theta\in\mathbb{R}$ is the parameter to be tested, and that $(U, T)$ is a sufficient statistic, where $U$ corresponds to $\theta$ and $T$ corresponds to all other parameters. Then UMP unbiased tests exist for most of the usual hypotheses, and can be written in the same way as in the one-parameter case, except that now all constants specifying the rejection region depend on $T$ (e.g. the rejection rule has the form $u &gt; C(t)$, etc.). Also, the size of the test is measured conditional on $T$.</p>

<p>See Theorem 4.4.1 in TSH.</p></li>
<li><p><em>[UMP unbiased and independent of sufficient statistic]</em> UMP unbiased tests for multi-parameter exponential families, as discussed in the last point, are independent of $T$ if a number of additional conditions are satisfied. For example, assume that $V = h(U, T)$ is independent of $T$ (with $\theta = \theta\subscript{1}$ and $\theta = \theta\subscript{2}$) and that $h$ is increasing in $u$. Then a UMP unbiased test for a two-sided null hypothesis rejects if $C\subscript{1} &lt; v &lt; C\subscript{2}$, accepts if $v &lt; C\subscript{1}$ or $v &gt; C\subscript{2}$, etc.</p>

<p>See Theorem 5.1.1 in TSH for more.</p></li>
<li><p><em>[UMP invariant tests]</em> If the problem of testing $H : \Omega\subscript{0}$ vs. $K : \Omega\subscript{1}$ remains invariant under a finite group $G = \{g\subscript{1}, g\subscript{2}, \dots, g\subscript{N} \}$, then there exists a UMP invariant test that rejects when $\frac{\sum p\subscript{\overline{g}\subscript{i} \theta\subscript{1}} (x)}{\sum p\subscript{\overline{g}\subscript{i} \theta\subscript{0}} (x)} &gt; C$ (for any $\theta\subscript{0} \in \Omega\subscript{0}$ and any $\theta\subscript{1}$ in $\Omega\subscript{1}$). See Theorem 6.3.1 in TSH.</p></li>
<li><p><em>[Finite composite null]</em> When the null hypothesis specifies that $X$ is distributed according to one of finitely many densities $p\subscript{1}, p\subscript{2}, \dots, p\subscript{m}$, and the alternative hypothesis is $p\subscript{m+1}$, then there exists a test $\phi$ that maximizes $\int \phi p\subscript{m+1} d\mu$. For suitable constants $k\subscript{1}, k\subscript{2}, \dots, k\subscript{m}$, this test rejects the null if $p\subscript{m+1}(x) &gt; \sum\subscript{i=1}^m k\subscript{i} p\subscript{i}(x)$, it accepts the null if $p\subscript{m+1}(x) &lt; \sum\subscript{i=1}^m k\subscript{i} p\subscript{i}(x)$, and it satisfies $\int \phi p\subscript{i} d\mu \leq \alpha$ for $i = 1,2,\dots,m$.</p>

<p>See Theorem 3.6.1 and Corollary 3.6.1 in TSH for more detail.</p></li>
<li><p><em>[Least favorable distributions]</em>  Assume a setting similar to the one in the last point, except that the number of distributions under the null hypothesis does not need to be finite. That is, $H : f\subscript{\theta}, \theta \in \omega$ vs. $K : g$.
One can define a <em>least favorable</em> distribution $\Lambda$ over $\omega$ and assume that $\theta \sim \Lambda$. As $\Lambda$ is least favorable, one can expect that it leads to a hypothesis test that works best in the worst case (i.e. at values $\theta$ closest to $K$). Thus, $\Lambda$ will typically be a distribution of $H$ that is closest to $K$. In particular, one would have $\Lambda(\omega^\prime) = 1$ for some &quot;boundary region&quot; $\omega^\prime$ of $\omega$. Then a MP test $\phi$ exists. It rejects if $g(x) &gt; k \int f\subscript{\theta}(x) d\Lambda(\theta)$, accepts if $g(x) &lt; k \int f\subscript{\theta}(x) d\Lambda(\theta)$, and satisfies $\sup\subscript{\theta\in\omega} \mathrm{E}\subscript{\theta} \phi(X) = \alpha$.</p>

<p>See Theorem 3.8.1 and Corollary 3.8.1 in TSH for rigour and detail.</p></li>
<li><p><em>[Maximin tests]</em> The same approach can also be generalized to test $H : f\subscript{\theta}, \theta \in \omega$ vs. $K : f\subscript{\theta}, \theta \in \omega^\prime$. However, one has to ditch the UMP condition in favor of the condition that $\inf\subscript{\theta\in\omega^\prime} \mathrm{E}\subscript{\theta} \phi(X)$ is maximized under the constraint $\sup\subscript{\theta\in\omega} \mathrm{E}\subscript{\theta} \phi(X) \leq \alpha$. Such a test is called a <em>maximin</em> test, and is established in Theorem 8.1.1 and Corollary 8.1.1 in TSH. It rejects if $\int\subscript{\omega^\prime} f\subscript{\theta}(x) d\Lambda^\prime(\theta)(x) &gt; C \int\subscript{\omega} f\subscript{\theta}(x) d\Lambda(\theta)$, for suitably chosen distributions $\Lambda^\prime$ and $\Lambda$.</p>

<p>Further, Theorem 8.5.1 (Hunt-Stein) and Lemma 8.4.1 in TSH establish the existence of almost invariant tests satisfying the maximin property.</p></li>
</ol>

  </article>

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">If you find any mistake then please <a href="https://github.com/agisga/Lehmanns_TSH_and_TPE/issues">submit an issue in the corresponding github repository.</a></h2>

    <div class="footer-col-wrapper">
      <div class="footer-col  footer-col-1">
        <ul class="contact-list">
          <li>Some impressions from the books  "Testing Statistical Hypotheses" and "Theory of Point Estimation"</li>
          <li><a href="mailto:alexej.go@googlemail.com">alexej.go@googlemail.com</a></li>
        </ul>
      </div>

      <div class="footer-col  footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/agisga">
              <span class="icon  icon--github">
                <svg viewBox="0 0 16 16">
                  <path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/>
                </svg>
              </span>

              <span class="username">agisga</span>
            </a>
          </li>
          

        </ul>
      </div>

      <div class="footer-col  footer-col-3">
        <p class="text">A collection of random facts I observed while reading Lehmann's  "Testing Statistical Hypotheses" and "Theory of Point Estimation"
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
